{"cells":[{"cell_type":"markdown","metadata":{"id":"Ymf05TlF5QAU"},"source":["# Main Configs"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"zHVD0it245SC","executionInfo":{"status":"ok","timestamp":1650282395762,"user_tz":-120,"elapsed":5,"user":{"displayName":"Bastian B.","userId":"10033977703569380654"}}},"outputs":[],"source":["small_df = False\n","batch_size = 1\n","width, height = 300, 300"]},{"cell_type":"markdown","metadata":{"id":"4BC8oFZjyN_v"},"source":["# Setup and Imports"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22799,"status":"ok","timestamp":1650282424193,"user":{"displayName":"Bastian B.","userId":"10033977703569380654"},"user_tz":-120},"id":"L07xyJhpDajY","outputId":"329b79eb-937f-483e-a6ad-d8bfe5edacf4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":25585,"status":"ok","timestamp":1650282453509,"user":{"displayName":"Bastian B.","userId":"10033977703569380654"},"user_tz":-120},"id":"e_ov9EndF-XS"},"outputs":[],"source":["!mkdir /usr/lib/python3.7/metrics\n","!cp -R /content/drive/MyDrive/BA/Notebooks/2_Experiment/review_object_detection_metrics-main/src /usr/lib/python3.7/metrics/src"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"wo1xjbFjNOvW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650282469134,"user_tz":-120,"elapsed":15631,"user":{"displayName":"Bastian B.","userId":"10033977703569380654"}},"outputId":"af1c7249-6307-44c2-a75a-5d2ac26b9995"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting PyQt5\n","  Downloading PyQt5-5.15.6-cp36-abi3-manylinux1_x86_64.whl (8.3 MB)\n","\u001b[K     |████████████████████████████████| 8.3 MB 30.0 MB/s \n","\u001b[?25hCollecting PyQt5-Qt5>=5.15.2\n","  Downloading PyQt5_Qt5-5.15.2-py3-none-manylinux2014_x86_64.whl (59.9 MB)\n","\u001b[K     |████████████████████████████████| 59.9 MB 91.4 MB/s \n","\u001b[?25hCollecting PyQt5-sip<13,>=12.8\n","  Downloading PyQt5_sip-12.10.1-cp37-cp37m-manylinux1_x86_64.whl (338 kB)\n","\u001b[K     |████████████████████████████████| 338 kB 39.1 MB/s \n","\u001b[?25hInstalling collected packages: PyQt5-sip, PyQt5-Qt5, PyQt5\n","Successfully installed PyQt5-5.15.6 PyQt5-Qt5-5.15.2 PyQt5-sip-12.10.1\n","\u001b[K     |████████████████████████████████| 48 kB 5.5 MB/s \n","\u001b[K     |████████████████████████████████| 948 kB 73.2 MB/s \n","\u001b[K     |████████████████████████████████| 58 kB 7.9 MB/s \n","\u001b[K     |████████████████████████████████| 60 kB 9.6 MB/s \n","\u001b[K     |████████████████████████████████| 10.9 MB 59.7 MB/s \n","\u001b[K     |████████████████████████████████| 229 kB 90.4 MB/s \n","\u001b[K     |████████████████████████████████| 78 kB 9.6 MB/s \n","\u001b[K     |████████████████████████████████| 51 kB 9.3 MB/s \n","\u001b[?25h  Building wheel for typing (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.4.0 which is incompatible.\u001b[0m\n"]}],"source":["!pip install PyQt5\n","!pip install -qU torch_snippets"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"XJigMscbLLYm","executionInfo":{"status":"ok","timestamp":1650282478392,"user_tz":-120,"elapsed":9262,"user":{"displayName":"Bastian B.","userId":"10033977703569380654"}}},"outputs":[],"source":["import copy\n","import glob\n","import torch\n","import time\n","import statistics\n","import cv2\n","import pandas as pd\n","import IPython\n","\n","from os.path import join\n","from torch_snippets import *\n","from PIL import Image\n","from metrics.src.evaluators import coco_evaluator, pascal_voc_evaluator\n","from metrics.src.bounding_box import BoundingBox\n","from metrics.src.utils.enumerators import BBFormat, BBType, CoordinatesType, MethodAveragePrecision\n","from skimage import data\n","from skimage.color import rgb2hsv, rgb2luv, rgb2lab\n","from skimage.util import random_noise"]},{"cell_type":"code","source":["pd.set_option('display.max_columns', None)"],"metadata":{"id":"v68G-1jvOrNR","executionInfo":{"status":"ok","timestamp":1650282478392,"user_tz":-120,"elapsed":15,"user":{"displayName":"Bastian B.","userId":"10033977703569380654"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","execution_count":7,"metadata":{"id":"tbSWaqcrXVJA","executionInfo":{"status":"ok","timestamp":1650282478393,"user_tz":-120,"elapsed":14,"user":{"displayName":"Bastian B.","userId":"10033977703569380654"}}},"outputs":[],"source":["IMAGE_ROOT = '/content/drive/MyDrive/BA/dataset/bus-trucks/images'\n","OUTPUT_REPORTS = '/content/drive/MyDrive/BA/Notebooks/2_Experiment/Output_Reports'\n","OUTPUT_MODELS = '/content/drive/MyDrive/BA/Notebooks/2_Experiment/Output_Models'\n","output_training_report = 'faster_rcnn_mobilenetv3_large.csv'\n","output_testing_report = 'testing_faster_rcnn_mobilenetv3_large.xlsx'\n","output_model_name = 'faster_rcnn_mobilenetv3_large.pt'"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":2316,"status":"ok","timestamp":1650282480697,"user":{"displayName":"Bastian B.","userId":"10033977703569380654"},"user_tz":-120},"id":"8K9USH1CGFjf","outputId":"77752baf-0482-451d-88c0-509c884ac7fd"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["               filename  class      xmin      xmax      ymin      ymax\n","0  00013f14dd4e168f.jpg    Bus  0.287500  0.999375  0.194184  0.999062\n","1  0002914fa805e227.jpg  Truck  0.061250  0.966875  0.125399  0.974495\n","2  0005f203463a13a8.jpg  Truck  0.000000  0.700000  0.187778  0.998889\n","3  00066517f9d814f9.jpg  Truck  0.000000  0.588867  0.069892  0.998208\n","4  000812dcf304a8e7.jpg    Bus  0.059375  0.848750  0.029936  0.958660"],"text/html":["\n","  <div id=\"df-38050dff-f147-44e8-9cf5-87e3f9c28c4e\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>filename</th>\n","      <th>class</th>\n","      <th>xmin</th>\n","      <th>xmax</th>\n","      <th>ymin</th>\n","      <th>ymax</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>00013f14dd4e168f.jpg</td>\n","      <td>Bus</td>\n","      <td>0.287500</td>\n","      <td>0.999375</td>\n","      <td>0.194184</td>\n","      <td>0.999062</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0002914fa805e227.jpg</td>\n","      <td>Truck</td>\n","      <td>0.061250</td>\n","      <td>0.966875</td>\n","      <td>0.125399</td>\n","      <td>0.974495</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0005f203463a13a8.jpg</td>\n","      <td>Truck</td>\n","      <td>0.000000</td>\n","      <td>0.700000</td>\n","      <td>0.187778</td>\n","      <td>0.998889</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>00066517f9d814f9.jpg</td>\n","      <td>Truck</td>\n","      <td>0.000000</td>\n","      <td>0.588867</td>\n","      <td>0.069892</td>\n","      <td>0.998208</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>000812dcf304a8e7.jpg</td>\n","      <td>Bus</td>\n","      <td>0.059375</td>\n","      <td>0.848750</td>\n","      <td>0.029936</td>\n","      <td>0.958660</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-38050dff-f147-44e8-9cf5-87e3f9c28c4e')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-38050dff-f147-44e8-9cf5-87e3f9c28c4e button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-38050dff-f147-44e8-9cf5-87e3f9c28c4e');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":8}],"source":["df_train = pd.read_csv('/content/drive/MyDrive/BA/dataset/Experimente/df_80_20_train_tf.csv')\n","df_test = pd.read_csv('/content/drive/MyDrive/BA/dataset/Experimente/df_80_20_test_tf.csv')\n","df_test.head()"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1650282480697,"user":{"displayName":"Bastian B.","userId":"10033977703569380654"},"user_tz":-120},"id":"2wq9qa5eLQ1A","outputId":"708530db-a1cb-4701-a484-2db9a34af18e"},"outputs":[{"output_type":"stream","name":"stdout","text":["{1: 'Bus', 2: 'Truck', 0: 'background'}\n"]}],"source":["label2target = {l:t+1 for t,l in enumerate(df_train['class'].unique())}\n","label2target['background'] = 0\n","target2label = {t:l for l,t in label2target.items()}\n","label2target = {v: k for k, v in target2label.items()}\n","background_class = label2target['background']\n","num_classes = len(label2target)\n","\n","print(target2label)"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":1321,"status":"ok","timestamp":1650282482015,"user":{"displayName":"Bastian B.","userId":"10033977703569380654"},"user_tz":-120},"id":"oR_KeMMcCq8j","outputId":"883a5119-b46d-485a-a9c3-f4f3b00db8c1"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["   epoch      time      loss  loss_classifier  loss_box_reg  loss_objectness  \\\n","0      0  0.728047  1.733443         1.099451      0.527920         0.085122   \n","1      0  0.273834  1.333175         0.786677      0.492336         0.037746   \n","2      0  0.267423  0.900961         0.419017      0.368975         0.096653   \n","3      0  0.256926  0.808685         0.311878      0.428779         0.050934   \n","4      0  0.246711  0.790508         0.331843      0.381438         0.059388   \n","\n","   loss_rpn_box_reg  \n","0          0.020950  \n","1          0.016416  \n","2          0.016316  \n","3          0.017095  \n","4          0.017839  "],"text/html":["\n","  <div id=\"df-acaf7d5c-27f7-4106-9dda-5b5750461007\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>epoch</th>\n","      <th>time</th>\n","      <th>loss</th>\n","      <th>loss_classifier</th>\n","      <th>loss_box_reg</th>\n","      <th>loss_objectness</th>\n","      <th>loss_rpn_box_reg</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0.728047</td>\n","      <td>1.733443</td>\n","      <td>1.099451</td>\n","      <td>0.527920</td>\n","      <td>0.085122</td>\n","      <td>0.020950</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>0.273834</td>\n","      <td>1.333175</td>\n","      <td>0.786677</td>\n","      <td>0.492336</td>\n","      <td>0.037746</td>\n","      <td>0.016416</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>0.267423</td>\n","      <td>0.900961</td>\n","      <td>0.419017</td>\n","      <td>0.368975</td>\n","      <td>0.096653</td>\n","      <td>0.016316</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>0.256926</td>\n","      <td>0.808685</td>\n","      <td>0.311878</td>\n","      <td>0.428779</td>\n","      <td>0.050934</td>\n","      <td>0.017095</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>0.246711</td>\n","      <td>0.790508</td>\n","      <td>0.331843</td>\n","      <td>0.381438</td>\n","      <td>0.059388</td>\n","      <td>0.017839</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-acaf7d5c-27f7-4106-9dda-5b5750461007')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-acaf7d5c-27f7-4106-9dda-5b5750461007 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-acaf7d5c-27f7-4106-9dda-5b5750461007');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":10}],"source":["df_report = pd.read_csv(join(OUTPUT_REPORTS, output_training_report))\n","df_report.head()"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"Q2ryFbUNs85q","executionInfo":{"status":"ok","timestamp":1650282482015,"user_tz":-120,"elapsed":11,"user":{"displayName":"Bastian B.","userId":"10033977703569380654"}}},"outputs":[],"source":["def preprocess_image(img):\n","  img = torch.tensor(img).permute(2,0,1)\n","  return img.to(device).float()\n","\n","class OpenDataset(torch.utils.data.Dataset):\n","  w, h = width, height\n","  def __init__(self, df, image_dir=IMAGE_ROOT):\n","    self.image_dir = image_dir\n","    self.files = glob.glob(self.image_dir+'/*')\n","    self.df = df\n","    self.image_infos = df['filename'].unique()\n","\n","  def __getitem__(self, ix):\n","\n","    #filename\tclass\txmin\txmax\tymin\tymax\n","\n","    # # load images and masks\n","    image_id = self.image_infos[ix]\n","    img_path = find(image_id, self.files)\n","    img = Image.open(img_path).convert('RGB')\n","    img = np.array(img.resize((self.w, self.h), resample=Image.BILINEAR))/255.\n","\n","    data = self.df[self.df['filename'] == image_id]\n","    labels = data['class'].values.tolist()\n","    data = data[['xmin','ymin','xmax','ymax']].values\n","    data[:,[0,2]] *= self.w\n","    data[:,[1,3]] *= self.h\n","    boxes = data.astype(np.uint32).tolist() # convert to absolute coordinates\n","    # torch FRCNN expects ground truths as a dictionary of tensors\n","    target = {}\n","    target[\"boxes\"] = torch.Tensor(boxes).float()\n","    target[\"labels\"] = torch.Tensor([label2target[i] for i in labels]).long()\n","    img = preprocess_image(img)\n","\n","    return img, target\n","  \n","  @classmethod\n","  def get_image_meta(self, ix):\n","    image_id = self.image_infos[ix]\n","    img_path = find(image_id, self.files)\n","    img = Image.open(img_path).convert('RGB')\n","    img = np.array(img)/255.\n","\n","    return image_id, img.shape\n","\n","  def collate_fn(self, batch):\n","    return tuple(zip(*batch)) \n","\n","  def __len__(self):\n","    return len(self.image_infos)"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"N_7ROj0gwIU1","executionInfo":{"status":"ok","timestamp":1650282482016,"user_tz":-120,"elapsed":9,"user":{"displayName":"Bastian B.","userId":"10033977703569380654"}}},"outputs":[],"source":["if small_df:\n","  df_test = df_test[:50]"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"cHEx-uaWP_5G","executionInfo":{"status":"ok","timestamp":1650282581607,"user_tz":-120,"elapsed":99599,"user":{"displayName":"Bastian B.","userId":"10033977703569380654"}}},"outputs":[],"source":["test_ds = OpenDataset(df_test)\n","test_loader = DataLoader(test_ds, batch_size=batch_size, collate_fn=test_ds.collate_fn, drop_last=True)"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"tGiY3lyIFA4C","executionInfo":{"status":"ok","timestamp":1650282658585,"user_tz":-120,"elapsed":824,"user":{"displayName":"Bastian B.","userId":"10033977703569380654"}}},"outputs":[],"source":["import torchvision\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","\n","PATH = join(OUTPUT_MODELS, output_model_name)\n","\n","# Load\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=False)\n","in_features = model.roi_heads.box_predictor.cls_score.in_features\n","model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","model.load_state_dict(torch.load(PATH, map_location=device))\n","model.to(device);"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"Y7N1vClMVUx1","executionInfo":{"status":"ok","timestamp":1650282658587,"user_tz":-120,"elapsed":8,"user":{"displayName":"Bastian B.","userId":"10033977703569380654"}}},"outputs":[],"source":["from torchvision.ops import nms\n","def decode_output(output):\n","  'convert tensors to numpy arrays'\n","  bbs = output['boxes'].cpu().detach().numpy().astype(np.uint16)\n","  labels = np.array([target2label[i] for i in output['labels'].cpu().detach().numpy()])\n","  confs = output['scores'].cpu().detach().numpy()\n","  ixs = nms(torch.tensor(bbs.astype(np.float32)), torch.tensor(confs), 0.05)\n","  bbs, confs, labels = [tensor[ixs] for tensor in [bbs, confs, labels]]\n","\n","  if len(ixs) == 1:\n","      bbs, confs, labels = [np.array([tensor]) for tensor in [bbs, confs, labels]]\n","  return bbs.tolist(), confs.tolist(), labels.tolist()"]},{"cell_type":"markdown","metadata":{"id":"XCVKW0B_YKLN"},"source":["# Size Analysis"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2736408,"status":"ok","timestamp":1650285394988,"user":{"displayName":"Bastian B.","userId":"10033977703569380654"},"user_tz":-120},"id":"mVtlcTIPVaee","outputId":"55df0a1c-8fae-4b81-ad81-a2d3aa22f58d"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  \n","/usr/local/lib/python3.7/dist-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n","  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"]}],"source":["bounding_boxes_detected = []\n","bounding_boxes_gt = []\n","model.eval()\n","\n","for ix, (images, targets) in enumerate(test_loader):\n","  images = [torch.tensor(im).type(torch.FloatTensor).to(device) for im in images]\n","  outputs = model(images)\n","  outputs = decode_output(outputs[0])\n","\n","  #Detected\n","  for i, output in enumerate(outputs[0]):\n","    bb = BoundingBox(\n","      image_name        = str(ix),\n","      class_id          = outputs[2][i],\n","      coordinates       = output,\n","      type_coordinates  = CoordinatesType.ABSOLUTE,\n","      bb_type           = BBType.DETECTED,\n","      confidence        = outputs[1][i],\n","      format            = BBFormat.XYX2Y2\n","    )\n","    bounding_boxes_detected.append(bb)\n","\n","\n","  #Ground Truth\n","  for i, box in enumerate(targets[0]['boxes']):\n","    cls = target2label[int(targets[0]['labels'][i])]\n","    bb = BoundingBox(\n","      image_name        = str(ix),\n","      class_id          = cls,\n","      coordinates       = list(box),\n","      type_coordinates  = CoordinatesType.ABSOLUTE,\n","      bb_type           = BBType.GROUND_TRUTH,\n","      confidence        = None,\n","      format            = BBFormat.XYX2Y2\n","    )\n","    bounding_boxes_gt.append(bb)"]},{"cell_type":"markdown","metadata":{"id":"oOW8t1uOU9UG"},"source":[""]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7400,"status":"ok","timestamp":1650285402380,"user":{"displayName":"Bastian B.","userId":"10033977703569380654"},"user_tz":-120},"id":"-Cafjb_keoRd","outputId":"a8868b00-bf68-452b-fe79-fb9091b83c9e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'AP': 0.47214799710877564,\n"," 'AP50': 0.6718900663408666,\n"," 'AP75': 0.5438246020152973,\n"," 'APlarge': 0.7428003170143275,\n"," 'APmedium': 0.6666866200994588,\n"," 'APsmall': 0.30836254415050773,\n"," 'AR1': 0.45822851446469287,\n"," 'AR10': 0.5369689462437832,\n"," 'AR100': 0.5372385360194335,\n"," 'ARlarge': 0.7872773159144894,\n"," 'ARmedium': 0.7246354952237306,\n"," 'ARsmall': 0.3816877122680583}"]},"metadata":{},"execution_count":19}],"source":["coco_evaluator.get_coco_summary(\n","    groundtruth_bbs = bounding_boxes_gt,\n","    detected_bbs = bounding_boxes_detected,\n","    small_size = 146,\n","    medium_size = 228\n",")"]},{"cell_type":"markdown","metadata":{"id":"-XplV_SmU4a1"},"source":["# Full COCO"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1650285402381,"user":{"displayName":"Bastian B.","userId":"10033977703569380654"},"user_tz":-120},"id":"jMpnwblAvg0r","outputId":"8f0aed71-e1a3-45c0-d83c-364922bdc0ba"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                                      Bus  \\\n","class                                                                 Bus   \n","precision               [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...   \n","recall                  [0.00043233895373973193, 0.0008646779074794639...   \n","AP                                                                0.70179   \n","interpolated precision  [1.0, 1.0, 1.0, 1.0, 1.0, 0.997872340425532, 0...   \n","\n","                                                                    Truck  \n","class                                                               Truck  \n","precision               [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...  \n","recall                  [0.0003996802557953637, 0.0007993605115907274,...  \n","AP                                                               0.641991  \n","interpolated precision  [1.0, 1.0, 0.9864864864864865, 0.9800995024875...  "],"text/html":["\n","  <div id=\"df-e110d6b4-8bc5-4f59-bb95-5c07f213f104\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Bus</th>\n","      <th>Truck</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>class</th>\n","      <td>Bus</td>\n","      <td>Truck</td>\n","    </tr>\n","    <tr>\n","      <th>precision</th>\n","      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n","      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>recall</th>\n","      <td>[0.00043233895373973193, 0.0008646779074794639...</td>\n","      <td>[0.0003996802557953637, 0.0007993605115907274,...</td>\n","    </tr>\n","    <tr>\n","      <th>AP</th>\n","      <td>0.70179</td>\n","      <td>0.641991</td>\n","    </tr>\n","    <tr>\n","      <th>interpolated precision</th>\n","      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 0.997872340425532, 0...</td>\n","      <td>[1.0, 1.0, 0.9864864864864865, 0.9800995024875...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e110d6b4-8bc5-4f59-bb95-5c07f213f104')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-e110d6b4-8bc5-4f59-bb95-5c07f213f104 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-e110d6b4-8bc5-4f59-bb95-5c07f213f104');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":20}],"source":["df = coco_evaluator.get_coco_metrics(\n","        bounding_boxes_gt,\n","        bounding_boxes_detected,\n","        iou_threshold=0.5,\n","        area_range=(0, np.inf),\n","        max_dets=100,\n",")\n","df = pd.DataFrame.from_dict(df)\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"TOVY3AZhU1f0"},"source":["# PASCAL VOL"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":112},"executionInfo":{"elapsed":3662,"status":"ok","timestamp":1650285406039,"user":{"displayName":"Bastian B.","userId":"10033977703569380654"},"user_tz":-120},"id":"umuNe92_sDXL","outputId":"e254dbd7-04be-4d01-fc19-8e2dc2ae6a68"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                               per_class       mAP\n","Bus    {'precision': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1...  0.742928\n","Truck  {'precision': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1...  0.742928"],"text/html":["\n","  <div id=\"df-db8824ad-4392-44b9-80a6-1fb86d3702b3\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>per_class</th>\n","      <th>mAP</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Bus</th>\n","      <td>{'precision': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1...</td>\n","      <td>0.742928</td>\n","    </tr>\n","    <tr>\n","      <th>Truck</th>\n","      <td>{'precision': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1...</td>\n","      <td>0.742928</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-db8824ad-4392-44b9-80a6-1fb86d3702b3')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-db8824ad-4392-44b9-80a6-1fb86d3702b3 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-db8824ad-4392-44b9-80a6-1fb86d3702b3');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":21}],"source":["df = pascal_voc_evaluator.get_pascalvoc_metrics(\n","    bounding_boxes_gt,\n","    bounding_boxes_detected,\n","    iou_threshold= 0.1,\n","    method=MethodAveragePrecision.EVERY_POINT_INTERPOLATION\n",")\n","\n","df = pd.DataFrame.from_dict(df)\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"RnM0quJQYHJp"},"source":["# Color Analysis"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1650285406039,"user":{"displayName":"Bastian B.","userId":"10033977703569380654"},"user_tz":-120},"id":"bTgNQN5mVRpP"},"outputs":[],"source":["def image_light(img, thrshld = 127):\n","  if type(img) is torch.Tensor:\n","    img = img.cpu().permute(1,2,0).numpy()\n","  rgb = img * 255\n","  rgb = np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140])\n","  \n","  is_light = np.mean(rgb) > thrshld\n","\n","  # Light: 0\n","  # Dark: 1\n","\n","  # return 0 if is_light else 1\n","  return {'value': np.mean(rgb),\n","          'brightness': 'light' if is_light else 'dark'\n","          }\n","\n","def avg_img_color(img):\n","  if type(img) is torch.Tensor:\n","    img = img.cpu().permute(1,2,0).numpy()\n","  rgb_avg = img.mean(axis=(0, 1)) \n","  hsv_avg = rgb2hsv(img).mean(axis=(0, 1))\n","  luv_avg = rgb2luv(img).mean(axis=(0, 1))\n","  lab_avg = rgb2lab(img).mean(axis=(0, 1))\n","\n","  return {\n","    'rgb_avg_r': float(rgb_avg[0]),\n","    'rgb_avg_g': float(rgb_avg[1]),\n","    'rgb_avg_b': float(rgb_avg[2]),\n","    'hsv_avg_h': float(hsv_avg[0]),\n","    'hsv_avg_s': float(hsv_avg[1]),\n","    'hsv_avg_v': float(hsv_avg[2])\n","  }\n","\n","def get_sharpness(img):\n","  if type(img) is torch.Tensor:\n","    img = img.cpu().permute(1,2,0).numpy()\n","  i = np.dot(img[...,:3], [0.2989, 0.5870, 0.1140])\n","\n","  gy, gx = np.gradient(i)\n","  gnorm = np.sqrt(gx**2 + gy**2)\n","  sharpness = np.average(gnorm)\n","  return sharpness"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1650285406039,"user":{"displayName":"Bastian B.","userId":"10033977703569380654"},"user_tz":-120},"id":"Nl0iDKb_VRmH"},"outputs":[],"source":["def bb_intersection_over_union(boxA, boxB):\n","\n","  xA = max(boxA[0], boxB[0])\n","  yA = max(boxA[1], boxB[1])\n","  xB = min(boxA[2], boxB[2])\n","  yB = min(boxA[3], boxB[3])\n","\n","  interArea = (xB - xA) * (yB - yA)\n","\n","  boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n","  boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n","\n","  iou = interArea / float(boxAArea + boxBArea - interArea)\n","\n","  return iou\n","\n","\n","def detected(preds, gt, iou_threshold ):\n","  det = False\n","  max_conf = 0\n","  for i, pred in enumerate(preds):\n","    if pred[0] == gt[0]:\n","      if pred[1] == gt[1]:\n","        iuo = bb_intersection_over_union(pred[-4:], gt[-4:])\n","        if iuo >= iou_threshold:\n","          det = True\n","          if pred[2] > max_conf:\n","            max_conf = pred[2]\n","\n","  return det, max_conf"]},{"cell_type":"code","source":["def printProgressBar (iteration, total, prefix = '', suffix = '', decimals = 1, length = 100, fill = '█', printEnd = \"\\r\"):\n","    percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n","    filledLength = int(length * iteration // total)\n","    bar = fill * filledLength + '-' * (length - filledLength)\n","    string = f'\\r{prefix} |{bar}| {percent}% {suffix}'\n","    out.update(IPython.display.Pretty(string))\n","    # Print New Line on Complete\n","    if iteration == total: \n","        print()"],"metadata":{"id":"LQ3z_YNVBUh5","executionInfo":{"status":"ok","timestamp":1650285406040,"user_tz":-120,"elapsed":3,"user":{"displayName":"Bastian B.","userId":"10033977703569380654"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":245},"executionInfo":{"elapsed":2588912,"status":"ok","timestamp":1650287994949,"user":{"displayName":"Bastian B.","userId":"10033977703569380654"},"user_tz":-120},"id":"EEXba0bVVRjQ","outputId":"9ad1aaf2-f3d7-4edc-e015-f02d3ef3b227"},"outputs":[{"output_type":"display_data","data":{"text/plain":["\rProgress: |██████████████████████████████████████████████████| 100.0% Complete"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:164: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:314: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:391: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:468: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: RuntimeWarning: divide by zero encountered in true_divide\n","  del sys.path[0]\n"]},{"output_type":"stream","name":"stdout","text":["\n","CPU times: user 50min 58s, sys: 1min 14s, total: 52min 13s\n","Wall time: 43min 8s\n"]}],"source":["%%time\n","\n","blur_filter = 1/9 *  torch.tensor([[1, 1, 1], [1, 1, 1], [1, 1, 1]])\n","metadata = []\n","n_items = len(test_loader)\n","out = display(IPython.display.Pretty('Starting'), display_id=True)\n","\n","model.eval()\n","\n","for ix, (images, targets) in enumerate(test_loader):\n","  # if ix == 10:\n","  #   break\n","  img = images[0]\n","  t = targets[0]['boxes']\n","  img_ = Image.fromarray(np.uint8(img.cpu().permute(1, 2, 0).numpy() * 255))\n","\n","\n","  # plain image\n","  c_img = image_light(img)\n","  c_avg_img = avg_img_color(img)\n","\n","  preds = []\n","  outputs = model(images)\n","  for i_output, output in enumerate(outputs):\n","    bbs, confs, labels = decode_output(output)\n","    for i_bb, bb in enumerate(bbs):\n","      X1, Y1, X2, Y2 = bb\n","      preds.append([ix, labels[i_bb], confs[i_bb], X1, Y1, X2, Y2])\n","\n","\n","  for i, box in enumerate(targets[0]['boxes']):\n","    cls = target2label[int(targets[0]['labels'][i])]\n","\n","    X1, Y1, X2, Y2 = box.numpy().astype(int)\n","\n","    # object frame\n","    obj_frame = img[:3, Y1:Y2, X1:X2]\n","\n","    # colors\n","    c_obj = image_light(obj_frame)\n","    c_avg_obj = avg_img_color(obj_frame)\n","\n","    # Area\n","    bb = BoundingBox(\n","      image_name        = str(ix),\n","      class_id          = cls,\n","      coordinates       = list(box),\n","      type_coordinates  = CoordinatesType.ABSOLUTE,\n","      bb_type           = BBType.GROUND_TRUTH,\n","      confidence        = None,\n","      format            = BBFormat.XYX2Y2\n","    )\n","    \n","    # detected\n","    det, conf = detected(\n","        preds,\n","        [ix, cls, 1, X1, Y1, X2, Y2],\n","        0.6\n","    )\n","    metadata.append({'image_id': ix,\n","                    'image_version': 'standard',\n","                    'image_brightness_value': c_img['value'],\n","                    'image_brightness_interpretation': c_img['brightness'],\n","                    'image_rgb_avg_r': c_avg_img['rgb_avg_r'],\n","                    'image_rgb_avg_g': c_avg_img['rgb_avg_g'],\n","                    'image_rgb_avg_b': c_avg_img['rgb_avg_b'],\n","                    'image_hsv_avg_h': c_avg_img['hsv_avg_h'],\n","                    'image_hsv_avg_s': c_avg_img['hsv_avg_s'],\n","                    'image_hsv_avg_v': c_avg_img['hsv_avg_v'],\n","                    'object_brightness_value': c_obj['value'],\n","                    'object_brightness_interpretation': c_obj['brightness'],\n","                    'object_rgb_avg_r': c_avg_obj['rgb_avg_r'],\n","                    'object_rgb_avg_g': c_avg_obj['rgb_avg_g'],\n","                    'object_rgb_avg_b': c_avg_obj['rgb_avg_b'],\n","                    'object_hsv_avg_h': c_avg_obj['hsv_avg_h'],\n","                    'object_hsv_avg_s': c_avg_obj['hsv_avg_s'],\n","                    'object_hsv_avg_v': c_avg_obj['hsv_avg_v'],\n","                    'detected': det,\n","                    'confidence': conf,\n","                    'object_size': bb.get_area(),\n","                    'object_class': cls\n","                   }\n","                  )\n","###########################################################################################\n","\n","  # blurred image\n","  blurred_img = cv2.filter2D(img.cpu().permute(1, 2, 0).numpy(), -1, kernel = blur_filter.numpy())\n","  c_img = image_light(blurred_img)\n","  c_avg_img = avg_img_color(blurred_img)\n","\n","  blurred_imgs = [torch.from_numpy(blurred_img).permute(2, 0, 1)]\n","  blurred_imgs = [torch.tensor(im).type(torch.FloatTensor).to(device) for im in blurred_imgs]\n","  blurred_img = blurred_imgs[0]\n","\n","  preds = []\n","  outputs = model(blurred_imgs) \n","\n","  for i_output, output in enumerate(outputs):\n","    bbs, confs, labels = decode_output(output)\n","    for i_bb, bb in enumerate(bbs):\n","      X1, Y1, X2, Y2 = bb\n","      preds.append([ix, labels[i_bb], confs[i_bb], X1, Y1, X2, Y2])\n","\n","\n","  for i, box in enumerate(targets[0]['boxes']):\n","    cls = target2label[int(targets[0]['labels'][i])]\n","\n","    X1, Y1, X2, Y2 = box.numpy().astype(int)\n","\n","    # object frame\n","    obj_frame = blurred_img[:3, Y1:Y2, X1:X2]\n","\n","    # colors\n","    c_obj = image_light(obj_frame)\n","    c_avg_obj = avg_img_color(obj_frame)\n","\n","    # Area\n","    bb = BoundingBox(\n","      image_name        = str(ix),\n","      class_id          = cls,\n","      coordinates       = list(box),\n","      type_coordinates  = CoordinatesType.ABSOLUTE,\n","      bb_type           = BBType.GROUND_TRUTH,\n","      confidence        = None,\n","      format            = BBFormat.XYX2Y2\n","    )\n","    \n","    # detected\n","    det, conf = detected(\n","        preds,\n","        [ix, cls, 1, X1, Y1, X2, Y2],\n","        0.6\n","    )\n","    metadata.append({'image_id': ix,\n","                    'image_version': 'blurred',\n","                    'image_brightness_value': c_img['value'],\n","                    'image_brightness_interpretation': c_img['brightness'],\n","                    'image_rgb_avg_r': c_avg_img['rgb_avg_r'],\n","                    'image_rgb_avg_g': c_avg_img['rgb_avg_g'],\n","                    'image_rgb_avg_b': c_avg_img['rgb_avg_b'],\n","                    'image_hsv_avg_h': c_avg_img['hsv_avg_h'],\n","                    'image_hsv_avg_s': c_avg_img['hsv_avg_s'],\n","                    'image_hsv_avg_v': c_avg_img['hsv_avg_v'],\n","                    'object_brightness_value': c_obj['value'],\n","                    'object_brightness_interpretation': c_obj['brightness'],\n","                    'object_rgb_avg_r': c_avg_obj['rgb_avg_r'],\n","                    'object_rgb_avg_g': c_avg_obj['rgb_avg_g'],\n","                    'object_rgb_avg_b': c_avg_obj['rgb_avg_b'],\n","                    'object_hsv_avg_h': c_avg_obj['hsv_avg_h'],\n","                    'object_hsv_avg_s': c_avg_obj['hsv_avg_s'],\n","                    'object_hsv_avg_v': c_avg_obj['hsv_avg_v'],\n","                    'detected': det,\n","                    'confidence': conf,\n","                    'object_size': bb.get_area(),\n","                    'object_class': cls\n","                   }\n","                  )\n","###########################################################################################\n","  # noisy image\n","  noise_img = random_noise(img.cpu().permute(1, 2, 0).numpy(), mode='s&p',amount=0.05)\n","  c_img = image_light(noise_img)\n","  c_avg_img = avg_img_color(noise_img)\n","\n","  noisy_imgs = [torch.from_numpy(noise_img).permute(2, 0, 1)]\n","  noisy_imgs = [torch.tensor(im).type(torch.FloatTensor).to(device) for im in noisy_imgs]\n","  noise_img = noisy_imgs[0]\n","\n","  preds = []\n","  outputs = model(noisy_imgs) \n","\n","  for i_output, output in enumerate(outputs):\n","    bbs, confs, labels = decode_output(output)\n","    for i_bb, bb in enumerate(bbs):\n","      X1, Y1, X2, Y2 = bb\n","      preds.append([ix, labels[i_bb], confs[i_bb], X1, Y1, X2, Y2])\n","\n","\n","  for i, box in enumerate(targets[0]['boxes']):\n","    cls = target2label[int(targets[0]['labels'][i])]\n","\n","    X1, Y1, X2, Y2 = box.numpy().astype(int)\n","\n","    # object frame\n","    obj_frame = noise_img[:3, Y1:Y2, X1:X2]\n","\n","    # colors\n","    c_obj = image_light(obj_frame)\n","    c_avg_obj = avg_img_color(obj_frame)\n","\n","    # Area\n","    bb = BoundingBox(\n","      image_name        = str(ix),\n","      class_id          = cls,\n","      coordinates       = list(box),\n","      type_coordinates  = CoordinatesType.ABSOLUTE,\n","      bb_type           = BBType.GROUND_TRUTH,\n","      confidence        = None,\n","      format            = BBFormat.XYX2Y2\n","    )\n","    \n","    # detected\n","    det, conf = detected(\n","        preds,\n","        [ix, cls, 1, X1, Y1, X2, Y2],\n","        0.6\n","    )\n","    metadata.append({'image_id': ix,\n","                    'image_version': 'noisy',\n","                    'image_brightness_value': c_img['value'],\n","                    'image_brightness_interpretation': c_img['brightness'],\n","                    'image_rgb_avg_r': c_avg_img['rgb_avg_r'],\n","                    'image_rgb_avg_g': c_avg_img['rgb_avg_g'],\n","                    'image_rgb_avg_b': c_avg_img['rgb_avg_b'],\n","                    'image_hsv_avg_h': c_avg_img['hsv_avg_h'],\n","                    'image_hsv_avg_s': c_avg_img['hsv_avg_s'],\n","                    'image_hsv_avg_v': c_avg_img['hsv_avg_v'],\n","                    'object_brightness_value': c_obj['value'],\n","                    'object_brightness_interpretation': c_obj['brightness'],\n","                    'object_rgb_avg_r': c_avg_obj['rgb_avg_r'],\n","                    'object_rgb_avg_g': c_avg_obj['rgb_avg_g'],\n","                    'object_rgb_avg_b': c_avg_obj['rgb_avg_b'],\n","                    'object_hsv_avg_h': c_avg_obj['hsv_avg_h'],\n","                    'object_hsv_avg_s': c_avg_obj['hsv_avg_s'],\n","                    'object_hsv_avg_v': c_avg_obj['hsv_avg_v'],\n","                    'detected': det,\n","                    'confidence': conf,\n","                    'object_size': bb.get_area(),\n","                    'object_class': cls\n","                   }\n","                  )\n","###########################################################################################\n","  # bright image\n","\n","  new_image = PIL.ImageEnhance.Brightness(img_).enhance(2)\n","  bright_image = np.array(new_image) / 255\n","  bright_imgs = [torch.from_numpy(bright_image).permute(2, 0, 1)]\n","  bright_imgs = [torch.tensor(im).type(torch.FloatTensor).to(device) for im in bright_imgs]\n","  bright_img = bright_imgs[0]\n","\n","  c_img = image_light(bright_image)\n","  c_avg_img = avg_img_color(bright_image)\n","\n","\n","\n","  preds = []\n","  outputs = model(bright_imgs) \n","\n","  for i_output, output in enumerate(outputs):\n","    bbs, confs, labels = decode_output(output)\n","    for i_bb, bb in enumerate(bbs):\n","      X1, Y1, X2, Y2 = bb\n","      preds.append([ix, labels[i_bb], confs[i_bb], X1, Y1, X2, Y2])\n","\n","\n","  for i, box in enumerate(targets[0]['boxes']):\n","    cls = target2label[int(targets[0]['labels'][i])]\n","\n","    X1, Y1, X2, Y2 = box.numpy().astype(int)\n","\n","    # object frame\n","    obj_frame = bright_img[:3, Y1:Y2, X1:X2]\n","\n","    # colors\n","    c_obj = image_light(obj_frame)\n","    c_avg_obj = avg_img_color(obj_frame)\n","\n","    # Area\n","    bb = BoundingBox(\n","      image_name        = str(ix),\n","      class_id          = cls,\n","      coordinates       = list(box),\n","      type_coordinates  = CoordinatesType.ABSOLUTE,\n","      bb_type           = BBType.GROUND_TRUTH,\n","      confidence        = None,\n","      format            = BBFormat.XYX2Y2\n","    )\n","    \n","    # detected\n","    det, conf = detected(\n","        preds,\n","        [ix, cls, 1, X1, Y1, X2, Y2],\n","        0.6\n","    )\n","    metadata.append({'image_id': ix,\n","                    'image_version': 'bright',\n","                    'image_brightness_value': c_img['value'],\n","                    'image_brightness_interpretation': c_img['brightness'],\n","                    'image_rgb_avg_r': c_avg_img['rgb_avg_r'],\n","                    'image_rgb_avg_g': c_avg_img['rgb_avg_g'],\n","                    'image_rgb_avg_b': c_avg_img['rgb_avg_b'],\n","                    'image_hsv_avg_h': c_avg_img['hsv_avg_h'],\n","                    'image_hsv_avg_s': c_avg_img['hsv_avg_s'],\n","                    'image_hsv_avg_v': c_avg_img['hsv_avg_v'],\n","                    'object_brightness_value': c_obj['value'],\n","                    'object_brightness_interpretation': c_obj['brightness'],\n","                    'object_rgb_avg_r': c_avg_obj['rgb_avg_r'],\n","                    'object_rgb_avg_g': c_avg_obj['rgb_avg_g'],\n","                    'object_rgb_avg_b': c_avg_obj['rgb_avg_b'],\n","                    'object_hsv_avg_h': c_avg_obj['hsv_avg_h'],\n","                    'object_hsv_avg_s': c_avg_obj['hsv_avg_s'],\n","                    'object_hsv_avg_v': c_avg_obj['hsv_avg_v'],\n","                    'detected': det,\n","                    'confidence': conf,\n","                    'object_size': bb.get_area(),\n","                    'object_class': cls\n","                   }\n","                  )\n","    \n","###########################################################################################\n","  # contrast image\n","\n","  new_image = PIL.ImageEnhance.Contrast(img_).enhance(2)\n","  contrast_image = np.array(new_image) / 255\n","  contrast_imgs = [torch.from_numpy(contrast_image).permute(2, 0, 1)]\n","  contrast_imgs = [torch.tensor(im).type(torch.FloatTensor).to(device) for im in contrast_imgs]\n","  contrast_img = contrast_imgs[0]\n","\n","  c_img = image_light(contrast_image)\n","  c_avg_img = avg_img_color(contrast_image)\n","\n","\n","\n","  preds = []\n","  outputs = model(contrast_imgs)\n","\n","  for i_output, output in enumerate(outputs):\n","    bbs, confs, labels = decode_output(output)\n","    for i_bb, bb in enumerate(bbs):\n","      X1, Y1, X2, Y2 = bb\n","      preds.append([ix, labels[i_bb], confs[i_bb], X1, Y1, X2, Y2])\n","\n","\n","  for i, box in enumerate(targets[0]['boxes']):\n","    cls = target2label[int(targets[0]['labels'][i])]\n","\n","    X1, Y1, X2, Y2 = box.numpy().astype(int)\n","\n","    # object frame\n","    obj_frame = contrast_img[:3, Y1:Y2, X1:X2]\n","\n","    # colors\n","    c_obj = image_light(obj_frame)\n","    c_avg_obj = avg_img_color(obj_frame)\n","\n","    # Area\n","    bb = BoundingBox(\n","      image_name        = str(ix),\n","      class_id          = cls,\n","      coordinates       = list(box),\n","      type_coordinates  = CoordinatesType.ABSOLUTE,\n","      bb_type           = BBType.GROUND_TRUTH,\n","      confidence        = None,\n","      format            = BBFormat.XYX2Y2\n","    )\n","    \n","    # detected\n","    det, conf = detected(\n","        preds,\n","        [ix, cls, 1, X1, Y1, X2, Y2],\n","        0.6\n","    )\n","    metadata.append({'image_id': ix,\n","                    'image_version': 'contrast',\n","                    'image_brightness_value': c_img['value'],\n","                    'image_brightness_interpretation': c_img['brightness'],\n","                    'image_rgb_avg_r': c_avg_img['rgb_avg_r'],\n","                    'image_rgb_avg_g': c_avg_img['rgb_avg_g'],\n","                    'image_rgb_avg_b': c_avg_img['rgb_avg_b'],\n","                    'image_hsv_avg_h': c_avg_img['hsv_avg_h'],\n","                    'image_hsv_avg_s': c_avg_img['hsv_avg_s'],\n","                    'image_hsv_avg_v': c_avg_img['hsv_avg_v'],\n","                    'object_brightness_value': c_obj['value'],\n","                    'object_brightness_interpretation': c_obj['brightness'],\n","                    'object_rgb_avg_r': c_avg_obj['rgb_avg_r'],\n","                    'object_rgb_avg_g': c_avg_obj['rgb_avg_g'],\n","                    'object_rgb_avg_b': c_avg_obj['rgb_avg_b'],\n","                    'object_hsv_avg_h': c_avg_obj['hsv_avg_h'],\n","                    'object_hsv_avg_s': c_avg_obj['hsv_avg_s'],\n","                    'object_hsv_avg_v': c_avg_obj['hsv_avg_v'],\n","                    'detected': det,\n","                    'confidence': conf,\n","                    'object_size': bb.get_area(),\n","                    'object_class': cls\n","                   }\n","                  )\n","###########################################################################################\n","  # sharp image\n","\n","  new_image = PIL.ImageEnhance.Sharpness(img_).enhance(3)\n","  sharp_image = np.array(new_image) / 255\n","  sharp_imgs = [torch.from_numpy(sharp_image).permute(2, 0, 1)]\n","  sharp_imgs = [torch.tensor(im).type(torch.FloatTensor).to(device) for im in sharp_imgs]\n","  sharp_img = sharp_imgs[0]\n","\n","  c_img = image_light(sharp_image)\n","  c_avg_img = avg_img_color(sharp_image)\n","\n","\n","\n","  preds = []\n","  outputs = model(sharp_imgs)\n","\n","  for i_output, output in enumerate(outputs):\n","    bbs, confs, labels = decode_output(output)\n","    for i_bb, bb in enumerate(bbs):\n","      X1, Y1, X2, Y2 = bb\n","      preds.append([ix, labels[i_bb], confs[i_bb], X1, Y1, X2, Y2])\n","\n","\n","  for i, box in enumerate(targets[0]['boxes']):\n","    cls = target2label[int(targets[0]['labels'][i])]\n","\n","    X1, Y1, X2, Y2 = box.numpy().astype(int)\n","\n","    # object frame\n","    obj_frame = sharp_img[:3, Y1:Y2, X1:X2]\n","\n","    # colors\n","    c_obj = image_light(obj_frame)\n","    c_avg_obj = avg_img_color(obj_frame)\n","\n","    # Area\n","    bb = BoundingBox(\n","      image_name        = str(ix),\n","      class_id          = cls,\n","      coordinates       = list(box),\n","      type_coordinates  = CoordinatesType.ABSOLUTE,\n","      bb_type           = BBType.GROUND_TRUTH,\n","      confidence        = None,\n","      format            = BBFormat.XYX2Y2\n","    )\n","    \n","    # detected\n","    det, conf = detected(\n","        preds,\n","        [ix, cls, 1, X1, Y1, X2, Y2],\n","        0.6\n","    )\n","    metadata.append({'image_id': ix,\n","                    'image_version': 'sharp',\n","                    'image_brightness_value': c_img['value'],\n","                    'image_brightness_interpretation': c_img['brightness'],\n","                    'image_rgb_avg_r': c_avg_img['rgb_avg_r'],\n","                    'image_rgb_avg_g': c_avg_img['rgb_avg_g'],\n","                    'image_rgb_avg_b': c_avg_img['rgb_avg_b'],\n","                    'image_hsv_avg_h': c_avg_img['hsv_avg_h'],\n","                    'image_hsv_avg_s': c_avg_img['hsv_avg_s'],\n","                    'image_hsv_avg_v': c_avg_img['hsv_avg_v'],\n","                    'object_brightness_value': c_obj['value'],\n","                    'object_brightness_interpretation': c_obj['brightness'],\n","                    'object_rgb_avg_r': c_avg_obj['rgb_avg_r'],\n","                    'object_rgb_avg_g': c_avg_obj['rgb_avg_g'],\n","                    'object_rgb_avg_b': c_avg_obj['rgb_avg_b'],\n","                    'object_hsv_avg_h': c_avg_obj['hsv_avg_h'],\n","                    'object_hsv_avg_s': c_avg_obj['hsv_avg_s'],\n","                    'object_hsv_avg_v': c_avg_obj['hsv_avg_v'],\n","                    'detected': det,\n","                    'confidence': conf,\n","                    'object_size': bb.get_area(),\n","                    'object_class': cls\n","                   }\n","                  )\n","###########################################################################################\n","  # Dark image\n","\n","  new_image = PIL.ImageEnhance.Brightness(img_).enhance(0.4)\n","  dark_image = np.array(new_image) / 255\n","  dark_imgs = [torch.from_numpy(dark_image).permute(2, 0, 1)]\n","  dark_imgs = [torch.tensor(im).type(torch.FloatTensor).to(device) for im in dark_imgs]\n","  dark_img = dark_imgs[0]\n","\n","  c_img = image_light(dark_image)\n","  c_avg_img = avg_img_color(dark_image)\n","\n","\n","\n","  preds = []\n","  outputs = model(dark_imgs)\n","\n","  for i_output, output in enumerate(outputs):\n","    bbs, confs, labels = decode_output(output)\n","    for i_bb, bb in enumerate(bbs):\n","      X1, Y1, X2, Y2 = bb\n","      preds.append([ix, labels[i_bb], confs[i_bb], X1, Y1, X2, Y2])\n","\n","\n","  for i, box in enumerate(targets[0]['boxes']):\n","    cls = target2label[int(targets[0]['labels'][i])]\n","\n","    X1, Y1, X2, Y2 = box.numpy().astype(int)\n","\n","    # object frame\n","    obj_frame = dark_img[:3, Y1:Y2, X1:X2]\n","\n","    # colors\n","    c_obj = image_light(obj_frame)\n","    c_avg_obj = avg_img_color(obj_frame)\n","\n","    # Area\n","    bb = BoundingBox(\n","      image_name        = str(ix),\n","      class_id          = cls,\n","      coordinates       = list(box),\n","      type_coordinates  = CoordinatesType.ABSOLUTE,\n","      bb_type           = BBType.GROUND_TRUTH,\n","      confidence        = None,\n","      format            = BBFormat.XYX2Y2\n","    )\n","    \n","    # detected\n","    det, conf = detected(\n","        preds,\n","        [ix, cls, 1, X1, Y1, X2, Y2],\n","        0.6\n","    )\n","    metadata.append({'image_id': ix,\n","                    'image_version': 'dark',\n","                    'image_brightness_value': c_img['value'],\n","                    'image_brightness_interpretation': c_img['brightness'],\n","                    'image_rgb_avg_r': c_avg_img['rgb_avg_r'],\n","                    'image_rgb_avg_g': c_avg_img['rgb_avg_g'],\n","                    'image_rgb_avg_b': c_avg_img['rgb_avg_b'],\n","                    'image_hsv_avg_h': c_avg_img['hsv_avg_h'],\n","                    'image_hsv_avg_s': c_avg_img['hsv_avg_s'],\n","                    'image_hsv_avg_v': c_avg_img['hsv_avg_v'],\n","                    'object_brightness_value': c_obj['value'],\n","                    'object_brightness_interpretation': c_obj['brightness'],\n","                    'object_rgb_avg_r': c_avg_obj['rgb_avg_r'],\n","                    'object_rgb_avg_g': c_avg_obj['rgb_avg_g'],\n","                    'object_rgb_avg_b': c_avg_obj['rgb_avg_b'],\n","                    'object_hsv_avg_h': c_avg_obj['hsv_avg_h'],\n","                    'object_hsv_avg_s': c_avg_obj['hsv_avg_s'],\n","                    'object_hsv_avg_v': c_avg_obj['hsv_avg_v'],\n","                    'detected': det,\n","                    'confidence': conf,\n","                    'object_size': bb.get_area(),\n","                    'object_class': cls\n","                   }\n","                  )\n","  printProgressBar(ix + 1, n_items, prefix = 'Progress:', suffix = 'Complete', length = 50)"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":333},"executionInfo":{"elapsed":16784,"status":"ok","timestamp":1650288012104,"user":{"displayName":"Bastian B.","userId":"10033977703569380654"},"user_tz":-120},"id":"BAvFYUomVRgl","outputId":"85dc8c5d-31a5-4aa3-ee20-2fc4bb97cd5f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["       image_id image_version  image_brightness_value  \\\n","33698      3044        bright              134.776908   \n","33699      3044      contrast               95.425101   \n","33700      3044      contrast               95.425101   \n","33701      3044         sharp               82.461668   \n","33702      3044         sharp               82.461668   \n","33703      3044          dark               32.575656   \n","33704      3044          dark               32.575656   \n","\n","      image_brightness_interpretation  image_rgb_avg_r  image_rgb_avg_g  \\\n","33698                           light         0.552707         0.519490   \n","33699                            dark         0.405319         0.362328   \n","33700                            dark         0.405319         0.362328   \n","33701                            dark         0.343913         0.316154   \n","33702                            dark         0.343913         0.316154   \n","33703                            dark         0.136002         0.124840   \n","33704                            dark         0.136002         0.124840   \n","\n","       image_rgb_avg_b  image_hsv_avg_h  image_hsv_avg_s  image_hsv_avg_v  \\\n","33698         0.512211         0.254606         0.208871         0.568104   \n","33699         0.354209         0.183127         0.219872         0.419370   \n","33700         0.354209         0.183127         0.219872         0.419370   \n","33701         0.307026         0.338734         0.255500         0.356855   \n","33702         0.307026         0.338734         0.255500         0.356855   \n","33703         0.121187         0.313693         0.260449         0.140653   \n","33704         0.121187         0.313693         0.260449         0.140653   \n","\n","       object_brightness_value object_brightness_interpretation  \\\n","33698               219.471943                            light   \n","33699               198.343621                            light   \n","33700               175.065043                            light   \n","33701               151.929901                            light   \n","33702               137.296597                            light   \n","33703                60.404664                             dark   \n","33704                54.514213                             dark   \n","\n","       object_rgb_avg_r  object_rgb_avg_g  object_rgb_avg_b  object_hsv_avg_h  \\\n","33698          0.900724          0.844611          0.839133          0.215730   \n","33699          0.790550          0.769863          0.786081          0.294754   \n","33700          0.755075          0.657424          0.657277          0.318293   \n","33701          0.607832          0.591539          0.586746          0.379261   \n","33702          0.574613          0.523310          0.521784          0.416595   \n","33703          0.241760          0.235164          0.233139          0.361929   \n","33704          0.228268          0.207731          0.207138          0.402688   \n","\n","       object_hsv_avg_s  object_hsv_avg_v  detected  confidence  object_size  \\\n","33698          0.116299          0.918348      True    0.916979      10486.0   \n","33699          0.135345          0.818334     False    0.000000       2240.0   \n","33700          0.271466          0.785582      True    0.994445      10486.0   \n","33701          0.131465          0.630860     False    0.000000       2240.0   \n","33702          0.209666          0.603817      True    0.992101      10486.0   \n","33703          0.113127          0.249445     False    0.000000       2240.0   \n","33704          0.190001          0.238191      True    0.893495      10486.0   \n","\n","      object_class  \n","33698          Bus  \n","33699          Bus  \n","33700          Bus  \n","33701          Bus  \n","33702          Bus  \n","33703          Bus  \n","33704          Bus  "],"text/html":["\n","  <div id=\"df-a19b89e5-523b-4d09-bc56-c8645156a0d3\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>image_id</th>\n","      <th>image_version</th>\n","      <th>image_brightness_value</th>\n","      <th>image_brightness_interpretation</th>\n","      <th>image_rgb_avg_r</th>\n","      <th>image_rgb_avg_g</th>\n","      <th>image_rgb_avg_b</th>\n","      <th>image_hsv_avg_h</th>\n","      <th>image_hsv_avg_s</th>\n","      <th>image_hsv_avg_v</th>\n","      <th>object_brightness_value</th>\n","      <th>object_brightness_interpretation</th>\n","      <th>object_rgb_avg_r</th>\n","      <th>object_rgb_avg_g</th>\n","      <th>object_rgb_avg_b</th>\n","      <th>object_hsv_avg_h</th>\n","      <th>object_hsv_avg_s</th>\n","      <th>object_hsv_avg_v</th>\n","      <th>detected</th>\n","      <th>confidence</th>\n","      <th>object_size</th>\n","      <th>object_class</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>33698</th>\n","      <td>3044</td>\n","      <td>bright</td>\n","      <td>134.776908</td>\n","      <td>light</td>\n","      <td>0.552707</td>\n","      <td>0.519490</td>\n","      <td>0.512211</td>\n","      <td>0.254606</td>\n","      <td>0.208871</td>\n","      <td>0.568104</td>\n","      <td>219.471943</td>\n","      <td>light</td>\n","      <td>0.900724</td>\n","      <td>0.844611</td>\n","      <td>0.839133</td>\n","      <td>0.215730</td>\n","      <td>0.116299</td>\n","      <td>0.918348</td>\n","      <td>True</td>\n","      <td>0.916979</td>\n","      <td>10486.0</td>\n","      <td>Bus</td>\n","    </tr>\n","    <tr>\n","      <th>33699</th>\n","      <td>3044</td>\n","      <td>contrast</td>\n","      <td>95.425101</td>\n","      <td>dark</td>\n","      <td>0.405319</td>\n","      <td>0.362328</td>\n","      <td>0.354209</td>\n","      <td>0.183127</td>\n","      <td>0.219872</td>\n","      <td>0.419370</td>\n","      <td>198.343621</td>\n","      <td>light</td>\n","      <td>0.790550</td>\n","      <td>0.769863</td>\n","      <td>0.786081</td>\n","      <td>0.294754</td>\n","      <td>0.135345</td>\n","      <td>0.818334</td>\n","      <td>False</td>\n","      <td>0.000000</td>\n","      <td>2240.0</td>\n","      <td>Bus</td>\n","    </tr>\n","    <tr>\n","      <th>33700</th>\n","      <td>3044</td>\n","      <td>contrast</td>\n","      <td>95.425101</td>\n","      <td>dark</td>\n","      <td>0.405319</td>\n","      <td>0.362328</td>\n","      <td>0.354209</td>\n","      <td>0.183127</td>\n","      <td>0.219872</td>\n","      <td>0.419370</td>\n","      <td>175.065043</td>\n","      <td>light</td>\n","      <td>0.755075</td>\n","      <td>0.657424</td>\n","      <td>0.657277</td>\n","      <td>0.318293</td>\n","      <td>0.271466</td>\n","      <td>0.785582</td>\n","      <td>True</td>\n","      <td>0.994445</td>\n","      <td>10486.0</td>\n","      <td>Bus</td>\n","    </tr>\n","    <tr>\n","      <th>33701</th>\n","      <td>3044</td>\n","      <td>sharp</td>\n","      <td>82.461668</td>\n","      <td>dark</td>\n","      <td>0.343913</td>\n","      <td>0.316154</td>\n","      <td>0.307026</td>\n","      <td>0.338734</td>\n","      <td>0.255500</td>\n","      <td>0.356855</td>\n","      <td>151.929901</td>\n","      <td>light</td>\n","      <td>0.607832</td>\n","      <td>0.591539</td>\n","      <td>0.586746</td>\n","      <td>0.379261</td>\n","      <td>0.131465</td>\n","      <td>0.630860</td>\n","      <td>False</td>\n","      <td>0.000000</td>\n","      <td>2240.0</td>\n","      <td>Bus</td>\n","    </tr>\n","    <tr>\n","      <th>33702</th>\n","      <td>3044</td>\n","      <td>sharp</td>\n","      <td>82.461668</td>\n","      <td>dark</td>\n","      <td>0.343913</td>\n","      <td>0.316154</td>\n","      <td>0.307026</td>\n","      <td>0.338734</td>\n","      <td>0.255500</td>\n","      <td>0.356855</td>\n","      <td>137.296597</td>\n","      <td>light</td>\n","      <td>0.574613</td>\n","      <td>0.523310</td>\n","      <td>0.521784</td>\n","      <td>0.416595</td>\n","      <td>0.209666</td>\n","      <td>0.603817</td>\n","      <td>True</td>\n","      <td>0.992101</td>\n","      <td>10486.0</td>\n","      <td>Bus</td>\n","    </tr>\n","    <tr>\n","      <th>33703</th>\n","      <td>3044</td>\n","      <td>dark</td>\n","      <td>32.575656</td>\n","      <td>dark</td>\n","      <td>0.136002</td>\n","      <td>0.124840</td>\n","      <td>0.121187</td>\n","      <td>0.313693</td>\n","      <td>0.260449</td>\n","      <td>0.140653</td>\n","      <td>60.404664</td>\n","      <td>dark</td>\n","      <td>0.241760</td>\n","      <td>0.235164</td>\n","      <td>0.233139</td>\n","      <td>0.361929</td>\n","      <td>0.113127</td>\n","      <td>0.249445</td>\n","      <td>False</td>\n","      <td>0.000000</td>\n","      <td>2240.0</td>\n","      <td>Bus</td>\n","    </tr>\n","    <tr>\n","      <th>33704</th>\n","      <td>3044</td>\n","      <td>dark</td>\n","      <td>32.575656</td>\n","      <td>dark</td>\n","      <td>0.136002</td>\n","      <td>0.124840</td>\n","      <td>0.121187</td>\n","      <td>0.313693</td>\n","      <td>0.260449</td>\n","      <td>0.140653</td>\n","      <td>54.514213</td>\n","      <td>dark</td>\n","      <td>0.228268</td>\n","      <td>0.207731</td>\n","      <td>0.207138</td>\n","      <td>0.402688</td>\n","      <td>0.190001</td>\n","      <td>0.238191</td>\n","      <td>True</td>\n","      <td>0.893495</td>\n","      <td>10486.0</td>\n","      <td>Bus</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a19b89e5-523b-4d09-bc56-c8645156a0d3')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-a19b89e5-523b-4d09-bc56-c8645156a0d3 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-a19b89e5-523b-4d09-bc56-c8645156a0d3');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":26}],"source":["df = pd.DataFrame.from_dict(metadata)\n","df.to_excel(join(OUTPUT_REPORTS, output_testing_report))\n","df.tail(7)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"Testing_FasterRCNN_MobileNetV3-Large.ipynb","provenance":[],"authorship_tag":"ABX9TyMg0vAFMxawTqR5Tg0WK/L3"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}