{"cells":[{"cell_type":"markdown","metadata":{"id":"Ymf05TlF5QAU"},"source":["# Main Configs"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"zHVD0it245SC","executionInfo":{"status":"ok","timestamp":1651531770837,"user_tz":-120,"elapsed":7,"user":{"displayName":"Bastian B.","userId":"10033977703569380654"}}},"outputs":[],"source":["small_df = False\n","batch_size = 1\n","width, height = 300, 300"]},{"cell_type":"markdown","metadata":{"id":"4BC8oFZjyN_v"},"source":["# Setup and Imports"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18906,"status":"ok","timestamp":1651531795170,"user":{"displayName":"Bastian B.","userId":"10033977703569380654"},"user_tz":-120},"id":"L07xyJhpDajY","outputId":"8af8cbbe-0497-4423-8576-fa64f083a097"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":12260,"status":"ok","timestamp":1651531807425,"user":{"displayName":"Bastian B.","userId":"10033977703569380654"},"user_tz":-120},"id":"e_ov9EndF-XS"},"outputs":[],"source":["!mkdir /usr/lib/python3.7/metrics\n","!cp -R /content/drive/MyDrive/BA/Notebooks/2_Experiment/review_object_detection_metrics-main/src /usr/lib/python3.7/metrics/src"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"wo1xjbFjNOvW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651531825064,"user_tz":-120,"elapsed":17642,"user":{"displayName":"Bastian B.","userId":"10033977703569380654"}},"outputId":"0b1edaf6-b03d-410b-cf89-a247beeefbde"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting PyQt5\n","  Downloading PyQt5-5.15.6-cp36-abi3-manylinux1_x86_64.whl (8.3 MB)\n","\u001b[K     |████████████████████████████████| 8.3 MB 8.0 MB/s \n","\u001b[?25hCollecting PyQt5-Qt5>=5.15.2\n","  Downloading PyQt5_Qt5-5.15.2-py3-none-manylinux2014_x86_64.whl (59.9 MB)\n","\u001b[K     |████████████████████████████████| 59.9 MB 1.4 MB/s \n","\u001b[?25hCollecting PyQt5-sip<13,>=12.8\n","  Downloading PyQt5_sip-12.10.1-cp37-cp37m-manylinux1_x86_64.whl (338 kB)\n","\u001b[K     |████████████████████████████████| 338 kB 65.9 MB/s \n","\u001b[?25hInstalling collected packages: PyQt5-sip, PyQt5-Qt5, PyQt5\n","Successfully installed PyQt5-5.15.6 PyQt5-Qt5-5.15.2 PyQt5-sip-12.10.1\n","\u001b[K     |████████████████████████████████| 48 kB 3.6 MB/s \n","\u001b[K     |████████████████████████████████| 58 kB 7.6 MB/s \n","\u001b[K     |████████████████████████████████| 78 kB 8.8 MB/s \n","\u001b[K     |████████████████████████████████| 60 kB 5.7 MB/s \n","\u001b[K     |████████████████████████████████| 948 kB 52.3 MB/s \n","\u001b[K     |████████████████████████████████| 10.9 MB 85.9 MB/s \n","\u001b[K     |████████████████████████████████| 232 kB 84.7 MB/s \n","\u001b[K     |████████████████████████████████| 51 kB 7.7 MB/s \n","\u001b[?25h  Building wheel for typing (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.4.0 which is incompatible.\u001b[0m\n"]}],"source":["!pip install PyQt5\n","!pip install -qU torch_snippets"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"XJigMscbLLYm","executionInfo":{"status":"ok","timestamp":1651531830724,"user_tz":-120,"elapsed":5675,"user":{"displayName":"Bastian B.","userId":"10033977703569380654"}}},"outputs":[],"source":["import copy\n","import glob\n","import torch\n","import time\n","import statistics\n","import cv2\n","import pandas as pd\n","import IPython\n","\n","from os.path import join\n","from torch_snippets import *\n","from PIL import Image\n","from metrics.src.evaluators import coco_evaluator, pascal_voc_evaluator\n","from metrics.src.bounding_box import BoundingBox\n","from metrics.src.utils.enumerators import BBFormat, BBType, CoordinatesType, MethodAveragePrecision\n","from skimage import data\n","from skimage.color import rgb2hsv, rgb2luv, rgb2lab\n","from skimage.util import random_noise"]},{"cell_type":"code","source":["pd.set_option('display.max_columns', None)"],"metadata":{"id":"v68G-1jvOrNR","executionInfo":{"status":"ok","timestamp":1651531830726,"user_tz":-120,"elapsed":18,"user":{"displayName":"Bastian B.","userId":"10033977703569380654"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","execution_count":8,"metadata":{"id":"tbSWaqcrXVJA","executionInfo":{"status":"ok","timestamp":1651531830727,"user_tz":-120,"elapsed":16,"user":{"displayName":"Bastian B.","userId":"10033977703569380654"}}},"outputs":[],"source":["IMAGE_ROOT = '/content/drive/MyDrive/BA/dataset/bus-trucks/images'\n","OUTPUT_REPORTS = '/content/drive/MyDrive/BA/Notebooks/2_Experiment/Output_Reports'\n","OUTPUT_MODELS = '/content/drive/MyDrive/BA/Notebooks/2_Experiment/Output_Models'\n","output_training_report = 'faster_rcnn_resnet50.csv'\n","output_testing_report = 'testing_faster_rcnn_resnet50.xlsx'\n","output_model_name = 'faster_rcnn_resnet50.pt'"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":1139,"status":"ok","timestamp":1651531831851,"user":{"displayName":"Bastian B.","userId":"10033977703569380654"},"user_tz":-120},"id":"8K9USH1CGFjf","outputId":"d21f8947-83a5-4c05-9806-19ad5830bd40"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["               filename  class      xmin      xmax      ymin      ymax\n","0  00013f14dd4e168f.jpg    Bus  0.287500  0.999375  0.194184  0.999062\n","1  0002914fa805e227.jpg  Truck  0.061250  0.966875  0.125399  0.974495\n","2  0005f203463a13a8.jpg  Truck  0.000000  0.700000  0.187778  0.998889\n","3  00066517f9d814f9.jpg  Truck  0.000000  0.588867  0.069892  0.998208\n","4  000812dcf304a8e7.jpg    Bus  0.059375  0.848750  0.029936  0.958660"],"text/html":["\n","  <div id=\"df-528e8b88-25d5-4645-a627-81ba3564c9e6\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>filename</th>\n","      <th>class</th>\n","      <th>xmin</th>\n","      <th>xmax</th>\n","      <th>ymin</th>\n","      <th>ymax</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>00013f14dd4e168f.jpg</td>\n","      <td>Bus</td>\n","      <td>0.287500</td>\n","      <td>0.999375</td>\n","      <td>0.194184</td>\n","      <td>0.999062</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0002914fa805e227.jpg</td>\n","      <td>Truck</td>\n","      <td>0.061250</td>\n","      <td>0.966875</td>\n","      <td>0.125399</td>\n","      <td>0.974495</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0005f203463a13a8.jpg</td>\n","      <td>Truck</td>\n","      <td>0.000000</td>\n","      <td>0.700000</td>\n","      <td>0.187778</td>\n","      <td>0.998889</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>00066517f9d814f9.jpg</td>\n","      <td>Truck</td>\n","      <td>0.000000</td>\n","      <td>0.588867</td>\n","      <td>0.069892</td>\n","      <td>0.998208</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>000812dcf304a8e7.jpg</td>\n","      <td>Bus</td>\n","      <td>0.059375</td>\n","      <td>0.848750</td>\n","      <td>0.029936</td>\n","      <td>0.958660</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-528e8b88-25d5-4645-a627-81ba3564c9e6')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-528e8b88-25d5-4645-a627-81ba3564c9e6 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-528e8b88-25d5-4645-a627-81ba3564c9e6');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":9}],"source":["df_train = pd.read_csv('/content/drive/MyDrive/BA/dataset/Experimente/df_80_20_train_tf.csv')\n","df_test = pd.read_csv('/content/drive/MyDrive/BA/dataset/Experimente/df_80_20_test_tf.csv')\n","df_test.head()"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1651531831852,"user":{"displayName":"Bastian B.","userId":"10033977703569380654"},"user_tz":-120},"id":"2wq9qa5eLQ1A","outputId":"d6128fd0-c771-4e74-a647-20bc37cc9b4e"},"outputs":[{"output_type":"stream","name":"stdout","text":["{1: 'Bus', 2: 'Truck', 0: 'background'}\n"]}],"source":["label2target = {l:t+1 for t,l in enumerate(df_train['class'].unique())}\n","label2target['background'] = 0\n","target2label = {t:l for l,t in label2target.items()}\n","label2target = {v: k for k, v in target2label.items()}\n","background_class = label2target['background']\n","num_classes = len(label2target)\n","\n","print(target2label)"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":350,"status":"ok","timestamp":1651531832196,"user":{"displayName":"Bastian B.","userId":"10033977703569380654"},"user_tz":-120},"id":"oR_KeMMcCq8j","outputId":"e7ea8e2d-81bd-4670-b4cd-8332305529a4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["   epoch      time      loss  loss_classifier  loss_box_reg  loss_objectness  \\\n","0      0  0.705694  1.382308         1.062186      0.290084         0.021461   \n","1      0  0.648374  0.757160         0.471357      0.259897         0.019422   \n","2      0  0.636158  0.442531         0.211634      0.200191         0.023782   \n","3      0  0.666274  0.414231         0.205684      0.179366         0.019670   \n","4      0  0.643864  0.453596         0.254380      0.182365         0.008665   \n","\n","   loss_rpn_box_reg  \n","0          0.008578  \n","1          0.006484  \n","2          0.006924  \n","3          0.009511  \n","4          0.008186  "],"text/html":["\n","  <div id=\"df-567905f7-8fd8-4889-81d7-0fb4bbe4d6e6\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>epoch</th>\n","      <th>time</th>\n","      <th>loss</th>\n","      <th>loss_classifier</th>\n","      <th>loss_box_reg</th>\n","      <th>loss_objectness</th>\n","      <th>loss_rpn_box_reg</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0.705694</td>\n","      <td>1.382308</td>\n","      <td>1.062186</td>\n","      <td>0.290084</td>\n","      <td>0.021461</td>\n","      <td>0.008578</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>0.648374</td>\n","      <td>0.757160</td>\n","      <td>0.471357</td>\n","      <td>0.259897</td>\n","      <td>0.019422</td>\n","      <td>0.006484</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>0.636158</td>\n","      <td>0.442531</td>\n","      <td>0.211634</td>\n","      <td>0.200191</td>\n","      <td>0.023782</td>\n","      <td>0.006924</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>0.666274</td>\n","      <td>0.414231</td>\n","      <td>0.205684</td>\n","      <td>0.179366</td>\n","      <td>0.019670</td>\n","      <td>0.009511</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>0.643864</td>\n","      <td>0.453596</td>\n","      <td>0.254380</td>\n","      <td>0.182365</td>\n","      <td>0.008665</td>\n","      <td>0.008186</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-567905f7-8fd8-4889-81d7-0fb4bbe4d6e6')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-567905f7-8fd8-4889-81d7-0fb4bbe4d6e6 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-567905f7-8fd8-4889-81d7-0fb4bbe4d6e6');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":11}],"source":["df_report = pd.read_csv(join(OUTPUT_REPORTS, output_training_report))\n","df_report.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q2ryFbUNs85q"},"outputs":[],"source":["def preprocess_image(img):\n","  img = torch.tensor(img).permute(2,0,1)\n","  return img.to(device).float()\n","\n","class OpenDataset(torch.utils.data.Dataset):\n","  w, h = width, height\n","  def __init__(self, df, image_dir=IMAGE_ROOT):\n","    self.image_dir = image_dir\n","    self.files = glob.glob(self.image_dir+'/*')\n","    self.df = df\n","    self.image_infos = df['filename'].unique()\n","\n","  def __getitem__(self, ix):\n","\n","    #filename\tclass\txmin\txmax\tymin\tymax\n","\n","    # # load images and masks\n","    image_id = self.image_infos[ix]\n","    img_path = find(image_id, self.files)\n","    img = Image.open(img_path).convert('RGB')\n","    img = np.array(img.resize((self.w, self.h), resample=Image.BILINEAR))/255.\n","\n","    data = self.df[self.df['filename'] == image_id]\n","    labels = data['class'].values.tolist()\n","    data = data[['xmin','ymin','xmax','ymax']].values\n","    data[:,[0,2]] *= self.w\n","    data[:,[1,3]] *= self.h\n","    boxes = data.astype(np.uint32).tolist() # convert to absolute coordinates\n","    # torch FRCNN expects ground truths as a dictionary of tensors\n","    target = {}\n","    target[\"boxes\"] = torch.Tensor(boxes).float()\n","    target[\"labels\"] = torch.Tensor([label2target[i] for i in labels]).long()\n","    img = preprocess_image(img)\n","\n","    return img, target\n","  \n","  @classmethod\n","  def get_image_meta(self, ix):\n","    image_id = self.image_infos[ix]\n","    img_path = find(image_id, self.files)\n","    img = Image.open(img_path).convert('RGB')\n","    img = np.array(img)/255.\n","\n","    return image_id, img.shape\n","\n","  def collate_fn(self, batch):\n","    return tuple(zip(*batch)) \n","\n","  def __len__(self):\n","    return len(self.image_infos)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N_7ROj0gwIU1"},"outputs":[],"source":["if small_df:\n","  df_test = df_test[:50]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cHEx-uaWP_5G"},"outputs":[],"source":["test_ds = OpenDataset(df_test)\n","test_loader = DataLoader(test_ds, batch_size=batch_size, collate_fn=test_ds.collate_fn, drop_last=True)"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"tGiY3lyIFA4C","colab":{"base_uri":"https://localhost:8080/","height":66,"referenced_widgets":["febdb11b259b4933b8ebb8e49f7ef374","06154f4ccd18411cbfbe1c2384ec4d30","cb33a300719c401192ef3976ef6ce06d","8568257107ce45e9bcbf8e43a9e1f681","6a7af6aef63f4ba8a3de07432490ee47","f9112828294d495d8a36117cd45038b8","d0a15509f8904e209c0f8c7b3bc09532","61494534609946c986349f39860dead2","69910f928a7442dc9e2cd8ef2a752dc7","c2492054e9f042709d93116fe58a51d9","268977162a114b21ab2d46ad0ff0ed64"]},"executionInfo":{"status":"ok","timestamp":1651531846603,"user_tz":-120,"elapsed":14410,"user":{"displayName":"Bastian B.","userId":"10033977703569380654"}},"outputId":"bc670fa5-e6ac-4f60-f15a-69fa9d1a5d00"},"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0.00/97.8M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"febdb11b259b4933b8ebb8e49f7ef374"}},"metadata":{}}],"source":["import torchvision\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","\n","PATH = join(OUTPUT_MODELS, output_model_name)\n","\n","# Load\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False)\n","in_features = model.roi_heads.box_predictor.cls_score.in_features\n","model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","model.load_state_dict(torch.load(PATH, map_location=device))\n","model.to(device);"]},{"cell_type":"code","source":["def count_parameters(model):\n","  return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","count_parameters(model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VleGQWMA95k3","executionInfo":{"status":"ok","timestamp":1651531850995,"user_tz":-120,"elapsed":246,"user":{"displayName":"Bastian B.","userId":"10033977703569380654"}},"outputId":"f4c59051-ea79-4065-c27c-7f385703a397"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["41081886"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y7N1vClMVUx1"},"outputs":[],"source":["from torchvision.ops import nms\n","def decode_output(output):\n","  'convert tensors to numpy arrays'\n","  bbs = output['boxes'].cpu().detach().numpy().astype(np.uint16)\n","  labels = np.array([target2label[i] for i in output['labels'].cpu().detach().numpy()])\n","  confs = output['scores'].cpu().detach().numpy()\n","  ixs = nms(torch.tensor(bbs.astype(np.float32)), torch.tensor(confs), 0.05)\n","  bbs, confs, labels = [tensor[ixs] for tensor in [bbs, confs, labels]]\n","\n","  if len(ixs) == 1:\n","      bbs, confs, labels = [np.array([tensor]) for tensor in [bbs, confs, labels]]\n","  return bbs.tolist(), confs.tolist(), labels.tolist()"]},{"cell_type":"markdown","metadata":{"id":"XCVKW0B_YKLN"},"source":["# Size Analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3003424,"status":"ok","timestamp":1649869399616,"user":{"displayName":"Bastian B.","userId":"10033977703569380654"},"user_tz":-120},"id":"mVtlcTIPVaee","outputId":"9a9c6b82-32ff-4774-830a-ac29d415c908"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  \n"]}],"source":["bounding_boxes_detected = []\n","bounding_boxes_gt = []\n","model.eval()\n","\n","for ix, (images, targets) in enumerate(test_loader):\n","  images = [torch.tensor(im).type(torch.FloatTensor).to(device) for im in images]\n","  outputs = model(images)\n","  outputs = decode_output(outputs[0])\n","\n","  #Detected\n","  for i, output in enumerate(outputs[0]):\n","    bb = BoundingBox(\n","      image_name        = str(ix),\n","      class_id          = outputs[2][i],\n","      coordinates       = output,\n","      type_coordinates  = CoordinatesType.ABSOLUTE,\n","      bb_type           = BBType.DETECTED,\n","      confidence        = outputs[1][i],\n","      format            = BBFormat.XYX2Y2\n","    )\n","    bounding_boxes_detected.append(bb)\n","\n","\n","  #Ground Truth\n","  for i, box in enumerate(targets[0]['boxes']):\n","    cls = target2label[int(targets[0]['labels'][i])]\n","    bb = BoundingBox(\n","      image_name        = str(ix),\n","      class_id          = cls,\n","      coordinates       = list(box),\n","      type_coordinates  = CoordinatesType.ABSOLUTE,\n","      bb_type           = BBType.GROUND_TRUTH,\n","      confidence        = None,\n","      format            = BBFormat.XYX2Y2\n","    )\n","    bounding_boxes_gt.append(bb)"]},{"cell_type":"markdown","metadata":{"id":"oOW8t1uOU9UG"},"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7986,"status":"ok","timestamp":1649869407597,"user":{"displayName":"Bastian B.","userId":"10033977703569380654"},"user_tz":-120},"id":"-Cafjb_keoRd","outputId":"d681635c-ca4b-439b-83d2-db7fb7d8e5b6"},"outputs":[{"data":{"text/plain":["{'AP': 0.48964693580012586,\n"," 'AP50': 0.6704095474763399,\n"," 'AP75': 0.5532199286143303,\n"," 'APlarge': 0.7552512505405914,\n"," 'APmedium': 0.6808923562326576,\n"," 'APsmall': 0.3173464147435412,\n"," 'AR1': 0.4656972476493514,\n"," 'AR10': 0.5442547907199532,\n"," 'AR100': 0.5442547907199532,\n"," 'ARlarge': 0.8072071049322342,\n"," 'ARmedium': 0.7258911513323278,\n"," 'ARsmall': 0.38746764206428097}"]},"execution_count":79,"metadata":{},"output_type":"execute_result"}],"source":["coco_evaluator.get_coco_summary(\n","    groundtruth_bbs = bounding_boxes_gt,\n","    detected_bbs = bounding_boxes_detected,\n","    small_size = 146,\n","    medium_size = 228\n",")"]},{"cell_type":"markdown","metadata":{"id":"-XplV_SmU4a1"},"source":["# Full COCO"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1649869407597,"user":{"displayName":"Bastian B.","userId":"10033977703569380654"},"user_tz":-120},"id":"jMpnwblAvg0r","outputId":"06ce84b0-dc13-4af9-dc04-70e6941c724a"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-89791fc1-45d3-4126-ba77-5de935af9e30\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Bus</th>\n","      <th>Truck</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>class</th>\n","      <td>Bus</td>\n","      <td>Truck</td>\n","    </tr>\n","    <tr>\n","      <th>precision</th>\n","      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n","      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>recall</th>\n","      <td>[0.00043233895373973193, 0.0008646779074794639...</td>\n","      <td>[0.0003996802557953637, 0.0007993605115907274,...</td>\n","    </tr>\n","    <tr>\n","      <th>AP</th>\n","      <td>0.717846</td>\n","      <td>0.622973</td>\n","    </tr>\n","    <tr>\n","      <th>interpolated precision</th>\n","      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n","      <td>[1.0, 1.0, 1.0, 1.0, 0.9907407407407407, 0.989...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-89791fc1-45d3-4126-ba77-5de935af9e30')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-89791fc1-45d3-4126-ba77-5de935af9e30 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-89791fc1-45d3-4126-ba77-5de935af9e30');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["                                                                      Bus  \\\n","class                                                                 Bus   \n","precision               [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...   \n","recall                  [0.00043233895373973193, 0.0008646779074794639...   \n","AP                                                               0.717846   \n","interpolated precision  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...   \n","\n","                                                                    Truck  \n","class                                                               Truck  \n","precision               [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...  \n","recall                  [0.0003996802557953637, 0.0007993605115907274,...  \n","AP                                                               0.622973  \n","interpolated precision  [1.0, 1.0, 1.0, 1.0, 0.9907407407407407, 0.989...  "]},"execution_count":80,"metadata":{},"output_type":"execute_result"}],"source":["df = coco_evaluator.get_coco_metrics(\n","        bounding_boxes_gt,\n","        bounding_boxes_detected,\n","        iou_threshold=0.5,\n","        area_range=(0, np.inf),\n","        max_dets=100,\n",")\n","df = pd.DataFrame.from_dict(df)\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"TOVY3AZhU1f0"},"source":["# PASCAL VOL"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":112},"executionInfo":{"elapsed":4686,"status":"ok","timestamp":1649869412280,"user":{"displayName":"Bastian B.","userId":"10033977703569380654"},"user_tz":-120},"id":"umuNe92_sDXL","outputId":"2026a892-0191-4e53-883e-97bdc210b844"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-f9721023-a769-4341-9c09-39d6134a4b1f\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>per_class</th>\n","      <th>mAP</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Bus</th>\n","      <td>{'precision': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1...</td>\n","      <td>0.737576</td>\n","    </tr>\n","    <tr>\n","      <th>Truck</th>\n","      <td>{'precision': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1...</td>\n","      <td>0.737576</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f9721023-a769-4341-9c09-39d6134a4b1f')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-f9721023-a769-4341-9c09-39d6134a4b1f button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-f9721023-a769-4341-9c09-39d6134a4b1f');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["                                               per_class       mAP\n","Bus    {'precision': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1...  0.737576\n","Truck  {'precision': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1...  0.737576"]},"execution_count":81,"metadata":{},"output_type":"execute_result"}],"source":["df = pascal_voc_evaluator.get_pascalvoc_metrics(\n","    bounding_boxes_gt,\n","    bounding_boxes_detected,\n","    iou_threshold= 0.1,\n","    method=MethodAveragePrecision.EVERY_POINT_INTERPOLATION\n",")\n","\n","df = pd.DataFrame.from_dict(df)\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"RnM0quJQYHJp"},"source":["# Color Analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bTgNQN5mVRpP"},"outputs":[],"source":["def image_light(img, thrshld = 127):\n","  if type(img) is torch.Tensor:\n","    img = img.cpu().permute(1,2,0).numpy()\n","  rgb = img * 255\n","  rgb = np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140])\n","  \n","  is_light = np.mean(rgb) > thrshld\n","\n","  # Light: 0\n","  # Dark: 1\n","\n","  # return 0 if is_light else 1\n","  return {'value': np.mean(rgb),\n","          'brightness': 'light' if is_light else 'dark'\n","          }\n","\n","def avg_img_color(img):\n","  if type(img) is torch.Tensor:\n","    img = img.cpu().permute(1,2,0).numpy()\n","  rgb_avg = img.mean(axis=(0, 1)) \n","  hsv_avg = rgb2hsv(img).mean(axis=(0, 1))\n","  luv_avg = rgb2luv(img).mean(axis=(0, 1))\n","  lab_avg = rgb2lab(img).mean(axis=(0, 1))\n","\n","  return {\n","    'rgb_avg_r': float(rgb_avg[0]),\n","    'rgb_avg_g': float(rgb_avg[1]),\n","    'rgb_avg_b': float(rgb_avg[2]),\n","    'hsv_avg_h': float(hsv_avg[0]),\n","    'hsv_avg_s': float(hsv_avg[1]),\n","    'hsv_avg_v': float(hsv_avg[2])\n","  }\n","\n","def get_sharpness(img):\n","  if type(img) is torch.Tensor:\n","    img = img.cpu().permute(1,2,0).numpy()\n","  i = np.dot(img[...,:3], [0.2989, 0.5870, 0.1140])\n","\n","  gy, gx = np.gradient(i)\n","  gnorm = np.sqrt(gx**2 + gy**2)\n","  sharpness = np.average(gnorm)\n","  return sharpness"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nl0iDKb_VRmH"},"outputs":[],"source":["def bb_intersection_over_union(boxA, boxB):\n","\n","  xA = max(boxA[0], boxB[0])\n","  yA = max(boxA[1], boxB[1])\n","  xB = min(boxA[2], boxB[2])\n","  yB = min(boxA[3], boxB[3])\n","\n","  interArea = (xB - xA) * (yB - yA)\n","\n","  boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n","  boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n","\n","  iou = interArea / float(boxAArea + boxBArea - interArea)\n","\n","  return iou\n","\n","\n","def detected(preds, gt, iou_threshold ):\n","  det = False\n","  max_conf = 0\n","  for i, pred in enumerate(preds):\n","    if pred[0] == gt[0]:\n","      if pred[1] == gt[1]:\n","        iuo = bb_intersection_over_union(pred[-4:], gt[-4:])\n","        if iuo >= iou_threshold:\n","          det = True\n","          if pred[2] > max_conf:\n","            max_conf = pred[2]\n","\n","  return det, max_conf"]},{"cell_type":"code","source":["def printProgressBar (iteration, total, prefix = '', suffix = '', decimals = 1, length = 100, fill = '█', printEnd = \"\\r\"):\n","    percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n","    filledLength = int(length * iteration // total)\n","    bar = fill * filledLength + '-' * (length - filledLength)\n","    string = f'\\r{prefix} |{bar}| {percent}% {suffix}'\n","    out.update(IPython.display.Pretty(string))\n","    # Print New Line on Complete\n","    if iteration == total: \n","        print()"],"metadata":{"id":"LQ3z_YNVBUh5"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"executionInfo":{"elapsed":4505045,"status":"ok","timestamp":1650229414429,"user":{"displayName":"Bastian B.","userId":"10033977703569380654"},"user_tz":-120},"id":"EEXba0bVVRjQ","outputId":"bb1ff2fc-5a83-4716-c282-a82c224569aa"},"outputs":[{"output_type":"display_data","data":{"text/plain":["\rProgress: |██████████████████████████████████████████████████| 100.0% Complete"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:164: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:314: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:391: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:468: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"]},{"output_type":"stream","name":"stdout","text":["\n","CPU times: user 57min 50s, sys: 40.7 s, total: 58min 31s\n","Wall time: 1h 15min 3s\n"]}],"source":["%%time\n","\n","blur_filter = 1/9 *  torch.tensor([[1, 1, 1], [1, 1, 1], [1, 1, 1]])\n","metadata = []\n","n_items = len(test_loader)\n","out = display(IPython.display.Pretty('Starting'), display_id=True)\n","\n","model.eval()\n","\n","for ix, (images, targets) in enumerate(test_loader):\n","  # if ix == 10:\n","  #   break\n","  img = images[0]\n","  t = targets[0]['boxes']\n","  img_ = Image.fromarray(np.uint8(img.cpu().permute(1, 2, 0).numpy() * 255))\n","\n","\n","  # plain image\n","  c_img = image_light(img)\n","  c_avg_img = avg_img_color(img)\n","\n","  preds = []\n","  outputs = model(images)\n","  for i_output, output in enumerate(outputs):\n","    bbs, confs, labels = decode_output(output)\n","    for i_bb, bb in enumerate(bbs):\n","      X1, Y1, X2, Y2 = bb\n","      preds.append([ix, labels[i_bb], confs[i_bb], X1, Y1, X2, Y2])\n","\n","\n","  for i, box in enumerate(targets[0]['boxes']):\n","    cls = target2label[int(targets[0]['labels'][i])]\n","\n","    X1, Y1, X2, Y2 = box.numpy().astype(int)\n","\n","    # object frame\n","    obj_frame = img[:3, Y1:Y2, X1:X2]\n","\n","    # colors\n","    c_obj = image_light(obj_frame)\n","    c_avg_obj = avg_img_color(obj_frame)\n","\n","    # Area\n","    bb = BoundingBox(\n","      image_name        = str(ix),\n","      class_id          = cls,\n","      coordinates       = list(box),\n","      type_coordinates  = CoordinatesType.ABSOLUTE,\n","      bb_type           = BBType.GROUND_TRUTH,\n","      confidence        = None,\n","      format            = BBFormat.XYX2Y2\n","    )\n","    \n","    # detected\n","    det, conf = detected(\n","        preds,\n","        [ix, cls, 1, X1, Y1, X2, Y2],\n","        0.6\n","    )\n","    metadata.append({'image_id': ix,\n","                    'image_version': 'standard',\n","                    'image_brightness_value': c_img['value'],\n","                    'image_brightness_interpretation': c_img['brightness'],\n","                    'image_rgb_avg_r': c_avg_img['rgb_avg_r'],\n","                    'image_rgb_avg_g': c_avg_img['rgb_avg_g'],\n","                    'image_rgb_avg_b': c_avg_img['rgb_avg_b'],\n","                    'image_hsv_avg_h': c_avg_img['hsv_avg_h'],\n","                    'image_hsv_avg_s': c_avg_img['hsv_avg_s'],\n","                    'image_hsv_avg_v': c_avg_img['hsv_avg_v'],\n","                    'object_brightness_value': c_obj['value'],\n","                    'object_brightness_interpretation': c_obj['brightness'],\n","                    'object_rgb_avg_r': c_avg_obj['rgb_avg_r'],\n","                    'object_rgb_avg_g': c_avg_obj['rgb_avg_g'],\n","                    'object_rgb_avg_b': c_avg_obj['rgb_avg_b'],\n","                    'object_hsv_avg_h': c_avg_obj['hsv_avg_h'],\n","                    'object_hsv_avg_s': c_avg_obj['hsv_avg_s'],\n","                    'object_hsv_avg_v': c_avg_obj['hsv_avg_v'],\n","                    'detected': det,\n","                    'confidence': conf,\n","                    'object_size': bb.get_area(),\n","                    'object_class': cls\n","                   }\n","                  )\n","###########################################################################################\n","\n","  # blurred image\n","  blurred_img = cv2.filter2D(img.cpu().permute(1, 2, 0).numpy(), -1, kernel = blur_filter.numpy())\n","  c_img = image_light(blurred_img)\n","  c_avg_img = avg_img_color(blurred_img)\n","\n","  blurred_imgs = [torch.from_numpy(blurred_img).permute(2, 0, 1)]\n","  blurred_imgs = [torch.tensor(im).type(torch.FloatTensor).to(device) for im in blurred_imgs]\n","  blurred_img = blurred_imgs[0]\n","\n","  preds = []\n","  outputs = model(blurred_imgs) \n","\n","  for i_output, output in enumerate(outputs):\n","    bbs, confs, labels = decode_output(output)\n","    for i_bb, bb in enumerate(bbs):\n","      X1, Y1, X2, Y2 = bb\n","      preds.append([ix, labels[i_bb], confs[i_bb], X1, Y1, X2, Y2])\n","\n","\n","  for i, box in enumerate(targets[0]['boxes']):\n","    cls = target2label[int(targets[0]['labels'][i])]\n","\n","    X1, Y1, X2, Y2 = box.numpy().astype(int)\n","\n","    # object frame\n","    obj_frame = blurred_img[:3, Y1:Y2, X1:X2]\n","\n","    # colors\n","    c_obj = image_light(obj_frame)\n","    c_avg_obj = avg_img_color(obj_frame)\n","\n","    # Area\n","    bb = BoundingBox(\n","      image_name        = str(ix),\n","      class_id          = cls,\n","      coordinates       = list(box),\n","      type_coordinates  = CoordinatesType.ABSOLUTE,\n","      bb_type           = BBType.GROUND_TRUTH,\n","      confidence        = None,\n","      format            = BBFormat.XYX2Y2\n","    )\n","    \n","    # detected\n","    det, conf = detected(\n","        preds,\n","        [ix, cls, 1, X1, Y1, X2, Y2],\n","        0.6\n","    )\n","    metadata.append({'image_id': ix,\n","                    'image_version': 'blurred',\n","                    'image_brightness_value': c_img['value'],\n","                    'image_brightness_interpretation': c_img['brightness'],\n","                    'image_rgb_avg_r': c_avg_img['rgb_avg_r'],\n","                    'image_rgb_avg_g': c_avg_img['rgb_avg_g'],\n","                    'image_rgb_avg_b': c_avg_img['rgb_avg_b'],\n","                    'image_hsv_avg_h': c_avg_img['hsv_avg_h'],\n","                    'image_hsv_avg_s': c_avg_img['hsv_avg_s'],\n","                    'image_hsv_avg_v': c_avg_img['hsv_avg_v'],\n","                    'object_brightness_value': c_obj['value'],\n","                    'object_brightness_interpretation': c_obj['brightness'],\n","                    'object_rgb_avg_r': c_avg_obj['rgb_avg_r'],\n","                    'object_rgb_avg_g': c_avg_obj['rgb_avg_g'],\n","                    'object_rgb_avg_b': c_avg_obj['rgb_avg_b'],\n","                    'object_hsv_avg_h': c_avg_obj['hsv_avg_h'],\n","                    'object_hsv_avg_s': c_avg_obj['hsv_avg_s'],\n","                    'object_hsv_avg_v': c_avg_obj['hsv_avg_v'],\n","                    'detected': det,\n","                    'confidence': conf,\n","                    'object_size': bb.get_area(),\n","                    'object_class': cls\n","                   }\n","                  )\n","###########################################################################################\n","  # noisy image\n","  noise_img = random_noise(img.cpu().permute(1, 2, 0).numpy(), mode='s&p',amount=0.05)\n","  c_img = image_light(noise_img)\n","  c_avg_img = avg_img_color(noise_img)\n","\n","  noisy_imgs = [torch.from_numpy(noise_img).permute(2, 0, 1)]\n","  noisy_imgs = [torch.tensor(im).type(torch.FloatTensor).to(device) for im in noisy_imgs]\n","  noise_img = noisy_imgs[0]\n","\n","  preds = []\n","  outputs = model(noisy_imgs) \n","\n","  for i_output, output in enumerate(outputs):\n","    bbs, confs, labels = decode_output(output)\n","    for i_bb, bb in enumerate(bbs):\n","      X1, Y1, X2, Y2 = bb\n","      preds.append([ix, labels[i_bb], confs[i_bb], X1, Y1, X2, Y2])\n","\n","\n","  for i, box in enumerate(targets[0]['boxes']):\n","    cls = target2label[int(targets[0]['labels'][i])]\n","\n","    X1, Y1, X2, Y2 = box.numpy().astype(int)\n","\n","    # object frame\n","    obj_frame = noise_img[:3, Y1:Y2, X1:X2]\n","\n","    # colors\n","    c_obj = image_light(obj_frame)\n","    c_avg_obj = avg_img_color(obj_frame)\n","\n","    # Area\n","    bb = BoundingBox(\n","      image_name        = str(ix),\n","      class_id          = cls,\n","      coordinates       = list(box),\n","      type_coordinates  = CoordinatesType.ABSOLUTE,\n","      bb_type           = BBType.GROUND_TRUTH,\n","      confidence        = None,\n","      format            = BBFormat.XYX2Y2\n","    )\n","    \n","    # detected\n","    det, conf = detected(\n","        preds,\n","        [ix, cls, 1, X1, Y1, X2, Y2],\n","        0.6\n","    )\n","    metadata.append({'image_id': ix,\n","                    'image_version': 'noisy',\n","                    'image_brightness_value': c_img['value'],\n","                    'image_brightness_interpretation': c_img['brightness'],\n","                    'image_rgb_avg_r': c_avg_img['rgb_avg_r'],\n","                    'image_rgb_avg_g': c_avg_img['rgb_avg_g'],\n","                    'image_rgb_avg_b': c_avg_img['rgb_avg_b'],\n","                    'image_hsv_avg_h': c_avg_img['hsv_avg_h'],\n","                    'image_hsv_avg_s': c_avg_img['hsv_avg_s'],\n","                    'image_hsv_avg_v': c_avg_img['hsv_avg_v'],\n","                    'object_brightness_value': c_obj['value'],\n","                    'object_brightness_interpretation': c_obj['brightness'],\n","                    'object_rgb_avg_r': c_avg_obj['rgb_avg_r'],\n","                    'object_rgb_avg_g': c_avg_obj['rgb_avg_g'],\n","                    'object_rgb_avg_b': c_avg_obj['rgb_avg_b'],\n","                    'object_hsv_avg_h': c_avg_obj['hsv_avg_h'],\n","                    'object_hsv_avg_s': c_avg_obj['hsv_avg_s'],\n","                    'object_hsv_avg_v': c_avg_obj['hsv_avg_v'],\n","                    'detected': det,\n","                    'confidence': conf,\n","                    'object_size': bb.get_area(),\n","                    'object_class': cls\n","                   }\n","                  )\n","###########################################################################################\n","  # bright image\n","\n","  new_image = PIL.ImageEnhance.Brightness(img_).enhance(2)\n","  bright_image = np.array(new_image) / 255\n","  bright_imgs = [torch.from_numpy(bright_image).permute(2, 0, 1)]\n","  bright_imgs = [torch.tensor(im).type(torch.FloatTensor).to(device) for im in bright_imgs]\n","  bright_img = bright_imgs[0]\n","\n","  c_img = image_light(bright_image)\n","  c_avg_img = avg_img_color(bright_image)\n","\n","\n","\n","  preds = []\n","  outputs = model(bright_imgs) \n","\n","  for i_output, output in enumerate(outputs):\n","    bbs, confs, labels = decode_output(output)\n","    for i_bb, bb in enumerate(bbs):\n","      X1, Y1, X2, Y2 = bb\n","      preds.append([ix, labels[i_bb], confs[i_bb], X1, Y1, X2, Y2])\n","\n","\n","  for i, box in enumerate(targets[0]['boxes']):\n","    cls = target2label[int(targets[0]['labels'][i])]\n","\n","    X1, Y1, X2, Y2 = box.numpy().astype(int)\n","\n","    # object frame\n","    obj_frame = bright_img[:3, Y1:Y2, X1:X2]\n","\n","    # colors\n","    c_obj = image_light(obj_frame)\n","    c_avg_obj = avg_img_color(obj_frame)\n","\n","    # Area\n","    bb = BoundingBox(\n","      image_name        = str(ix),\n","      class_id          = cls,\n","      coordinates       = list(box),\n","      type_coordinates  = CoordinatesType.ABSOLUTE,\n","      bb_type           = BBType.GROUND_TRUTH,\n","      confidence        = None,\n","      format            = BBFormat.XYX2Y2\n","    )\n","    \n","    # detected\n","    det, conf = detected(\n","        preds,\n","        [ix, cls, 1, X1, Y1, X2, Y2],\n","        0.6\n","    )\n","    metadata.append({'image_id': ix,\n","                    'image_version': 'bright',\n","                    'image_brightness_value': c_img['value'],\n","                    'image_brightness_interpretation': c_img['brightness'],\n","                    'image_rgb_avg_r': c_avg_img['rgb_avg_r'],\n","                    'image_rgb_avg_g': c_avg_img['rgb_avg_g'],\n","                    'image_rgb_avg_b': c_avg_img['rgb_avg_b'],\n","                    'image_hsv_avg_h': c_avg_img['hsv_avg_h'],\n","                    'image_hsv_avg_s': c_avg_img['hsv_avg_s'],\n","                    'image_hsv_avg_v': c_avg_img['hsv_avg_v'],\n","                    'object_brightness_value': c_obj['value'],\n","                    'object_brightness_interpretation': c_obj['brightness'],\n","                    'object_rgb_avg_r': c_avg_obj['rgb_avg_r'],\n","                    'object_rgb_avg_g': c_avg_obj['rgb_avg_g'],\n","                    'object_rgb_avg_b': c_avg_obj['rgb_avg_b'],\n","                    'object_hsv_avg_h': c_avg_obj['hsv_avg_h'],\n","                    'object_hsv_avg_s': c_avg_obj['hsv_avg_s'],\n","                    'object_hsv_avg_v': c_avg_obj['hsv_avg_v'],\n","                    'detected': det,\n","                    'confidence': conf,\n","                    'object_size': bb.get_area(),\n","                    'object_class': cls\n","                   }\n","                  )\n","    \n","###########################################################################################\n","  # contrast image\n","\n","  new_image = PIL.ImageEnhance.Contrast(img_).enhance(2)\n","  contrast_image = np.array(new_image) / 255\n","  contrast_imgs = [torch.from_numpy(contrast_image).permute(2, 0, 1)]\n","  contrast_imgs = [torch.tensor(im).type(torch.FloatTensor).to(device) for im in contrast_imgs]\n","  contrast_img = contrast_imgs[0]\n","\n","  c_img = image_light(contrast_image)\n","  c_avg_img = avg_img_color(contrast_image)\n","\n","\n","\n","  preds = []\n","  outputs = model(contrast_imgs)\n","\n","  for i_output, output in enumerate(outputs):\n","    bbs, confs, labels = decode_output(output)\n","    for i_bb, bb in enumerate(bbs):\n","      X1, Y1, X2, Y2 = bb\n","      preds.append([ix, labels[i_bb], confs[i_bb], X1, Y1, X2, Y2])\n","\n","\n","  for i, box in enumerate(targets[0]['boxes']):\n","    cls = target2label[int(targets[0]['labels'][i])]\n","\n","    X1, Y1, X2, Y2 = box.numpy().astype(int)\n","\n","    # object frame\n","    obj_frame = contrast_img[:3, Y1:Y2, X1:X2]\n","\n","    # colors\n","    c_obj = image_light(obj_frame)\n","    c_avg_obj = avg_img_color(obj_frame)\n","\n","    # Area\n","    bb = BoundingBox(\n","      image_name        = str(ix),\n","      class_id          = cls,\n","      coordinates       = list(box),\n","      type_coordinates  = CoordinatesType.ABSOLUTE,\n","      bb_type           = BBType.GROUND_TRUTH,\n","      confidence        = None,\n","      format            = BBFormat.XYX2Y2\n","    )\n","    \n","    # detected\n","    det, conf = detected(\n","        preds,\n","        [ix, cls, 1, X1, Y1, X2, Y2],\n","        0.6\n","    )\n","    metadata.append({'image_id': ix,\n","                    'image_version': 'contrast',\n","                    'image_brightness_value': c_img['value'],\n","                    'image_brightness_interpretation': c_img['brightness'],\n","                    'image_rgb_avg_r': c_avg_img['rgb_avg_r'],\n","                    'image_rgb_avg_g': c_avg_img['rgb_avg_g'],\n","                    'image_rgb_avg_b': c_avg_img['rgb_avg_b'],\n","                    'image_hsv_avg_h': c_avg_img['hsv_avg_h'],\n","                    'image_hsv_avg_s': c_avg_img['hsv_avg_s'],\n","                    'image_hsv_avg_v': c_avg_img['hsv_avg_v'],\n","                    'object_brightness_value': c_obj['value'],\n","                    'object_brightness_interpretation': c_obj['brightness'],\n","                    'object_rgb_avg_r': c_avg_obj['rgb_avg_r'],\n","                    'object_rgb_avg_g': c_avg_obj['rgb_avg_g'],\n","                    'object_rgb_avg_b': c_avg_obj['rgb_avg_b'],\n","                    'object_hsv_avg_h': c_avg_obj['hsv_avg_h'],\n","                    'object_hsv_avg_s': c_avg_obj['hsv_avg_s'],\n","                    'object_hsv_avg_v': c_avg_obj['hsv_avg_v'],\n","                    'detected': det,\n","                    'confidence': conf,\n","                    'object_size': bb.get_area(),\n","                    'object_class': cls\n","                   }\n","                  )\n","###########################################################################################\n","  # sharp image\n","\n","  new_image = PIL.ImageEnhance.Sharpness(img_).enhance(3)\n","  sharp_image = np.array(new_image) / 255\n","  sharp_imgs = [torch.from_numpy(sharp_image).permute(2, 0, 1)]\n","  sharp_imgs = [torch.tensor(im).type(torch.FloatTensor).to(device) for im in sharp_imgs]\n","  sharp_img = sharp_imgs[0]\n","\n","  c_img = image_light(sharp_image)\n","  c_avg_img = avg_img_color(sharp_image)\n","\n","\n","\n","  preds = []\n","  outputs = model(sharp_imgs)\n","\n","  for i_output, output in enumerate(outputs):\n","    bbs, confs, labels = decode_output(output)\n","    for i_bb, bb in enumerate(bbs):\n","      X1, Y1, X2, Y2 = bb\n","      preds.append([ix, labels[i_bb], confs[i_bb], X1, Y1, X2, Y2])\n","\n","\n","  for i, box in enumerate(targets[0]['boxes']):\n","    cls = target2label[int(targets[0]['labels'][i])]\n","\n","    X1, Y1, X2, Y2 = box.numpy().astype(int)\n","\n","    # object frame\n","    obj_frame = sharp_img[:3, Y1:Y2, X1:X2]\n","\n","    # colors\n","    c_obj = image_light(obj_frame)\n","    c_avg_obj = avg_img_color(obj_frame)\n","\n","    # Area\n","    bb = BoundingBox(\n","      image_name        = str(ix),\n","      class_id          = cls,\n","      coordinates       = list(box),\n","      type_coordinates  = CoordinatesType.ABSOLUTE,\n","      bb_type           = BBType.GROUND_TRUTH,\n","      confidence        = None,\n","      format            = BBFormat.XYX2Y2\n","    )\n","    \n","    # detected\n","    det, conf = detected(\n","        preds,\n","        [ix, cls, 1, X1, Y1, X2, Y2],\n","        0.6\n","    )\n","    metadata.append({'image_id': ix,\n","                    'image_version': 'sharp',\n","                    'image_brightness_value': c_img['value'],\n","                    'image_brightness_interpretation': c_img['brightness'],\n","                    'image_rgb_avg_r': c_avg_img['rgb_avg_r'],\n","                    'image_rgb_avg_g': c_avg_img['rgb_avg_g'],\n","                    'image_rgb_avg_b': c_avg_img['rgb_avg_b'],\n","                    'image_hsv_avg_h': c_avg_img['hsv_avg_h'],\n","                    'image_hsv_avg_s': c_avg_img['hsv_avg_s'],\n","                    'image_hsv_avg_v': c_avg_img['hsv_avg_v'],\n","                    'object_brightness_value': c_obj['value'],\n","                    'object_brightness_interpretation': c_obj['brightness'],\n","                    'object_rgb_avg_r': c_avg_obj['rgb_avg_r'],\n","                    'object_rgb_avg_g': c_avg_obj['rgb_avg_g'],\n","                    'object_rgb_avg_b': c_avg_obj['rgb_avg_b'],\n","                    'object_hsv_avg_h': c_avg_obj['hsv_avg_h'],\n","                    'object_hsv_avg_s': c_avg_obj['hsv_avg_s'],\n","                    'object_hsv_avg_v': c_avg_obj['hsv_avg_v'],\n","                    'detected': det,\n","                    'confidence': conf,\n","                    'object_size': bb.get_area(),\n","                    'object_class': cls\n","                   }\n","                  )\n","###########################################################################################\n","  # Dark image\n","\n","  new_image = PIL.ImageEnhance.Brightness(img_).enhance(0.4)\n","  dark_image = np.array(new_image) / 255\n","  dark_imgs = [torch.from_numpy(dark_image).permute(2, 0, 1)]\n","  dark_imgs = [torch.tensor(im).type(torch.FloatTensor).to(device) for im in dark_imgs]\n","  dark_img = dark_imgs[0]\n","\n","  c_img = image_light(dark_image)\n","  c_avg_img = avg_img_color(dark_image)\n","\n","\n","\n","  preds = []\n","  outputs = model(dark_imgs)\n","\n","  for i_output, output in enumerate(outputs):\n","    bbs, confs, labels = decode_output(output)\n","    for i_bb, bb in enumerate(bbs):\n","      X1, Y1, X2, Y2 = bb\n","      preds.append([ix, labels[i_bb], confs[i_bb], X1, Y1, X2, Y2])\n","\n","\n","  for i, box in enumerate(targets[0]['boxes']):\n","    cls = target2label[int(targets[0]['labels'][i])]\n","\n","    X1, Y1, X2, Y2 = box.numpy().astype(int)\n","\n","    # object frame\n","    obj_frame = dark_img[:3, Y1:Y2, X1:X2]\n","\n","    # colors\n","    c_obj = image_light(obj_frame)\n","    c_avg_obj = avg_img_color(obj_frame)\n","\n","    # Area\n","    bb = BoundingBox(\n","      image_name        = str(ix),\n","      class_id          = cls,\n","      coordinates       = list(box),\n","      type_coordinates  = CoordinatesType.ABSOLUTE,\n","      bb_type           = BBType.GROUND_TRUTH,\n","      confidence        = None,\n","      format            = BBFormat.XYX2Y2\n","    )\n","    \n","    # detected\n","    det, conf = detected(\n","        preds,\n","        [ix, cls, 1, X1, Y1, X2, Y2],\n","        0.6\n","    )\n","    metadata.append({'image_id': ix,\n","                    'image_version': 'dark',\n","                    'image_brightness_value': c_img['value'],\n","                    'image_brightness_interpretation': c_img['brightness'],\n","                    'image_rgb_avg_r': c_avg_img['rgb_avg_r'],\n","                    'image_rgb_avg_g': c_avg_img['rgb_avg_g'],\n","                    'image_rgb_avg_b': c_avg_img['rgb_avg_b'],\n","                    'image_hsv_avg_h': c_avg_img['hsv_avg_h'],\n","                    'image_hsv_avg_s': c_avg_img['hsv_avg_s'],\n","                    'image_hsv_avg_v': c_avg_img['hsv_avg_v'],\n","                    'object_brightness_value': c_obj['value'],\n","                    'object_brightness_interpretation': c_obj['brightness'],\n","                    'object_rgb_avg_r': c_avg_obj['rgb_avg_r'],\n","                    'object_rgb_avg_g': c_avg_obj['rgb_avg_g'],\n","                    'object_rgb_avg_b': c_avg_obj['rgb_avg_b'],\n","                    'object_hsv_avg_h': c_avg_obj['hsv_avg_h'],\n","                    'object_hsv_avg_s': c_avg_obj['hsv_avg_s'],\n","                    'object_hsv_avg_v': c_avg_obj['hsv_avg_v'],\n","                    'detected': det,\n","                    'confidence': conf,\n","                    'object_size': bb.get_area(),\n","                    'object_class': cls\n","                   }\n","                  )\n","  printProgressBar(ix + 1, n_items, prefix = 'Progress:', suffix = 'Complete', length = 50)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":333},"executionInfo":{"elapsed":21609,"status":"ok","timestamp":1650230449469,"user":{"displayName":"Bastian B.","userId":"10033977703569380654"},"user_tz":-120},"id":"BAvFYUomVRgl","outputId":"be27b40b-20d1-4e4d-b6ae-cae2e4993113"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["       image_id image_version  image_brightness_value  \\\n","33698      3044        bright              134.776908   \n","33699      3044      contrast               95.425101   \n","33700      3044      contrast               95.425101   \n","33701      3044         sharp               82.461668   \n","33702      3044         sharp               82.461668   \n","33703      3044          dark               32.575656   \n","33704      3044          dark               32.575656   \n","\n","      image_brightness_interpretation  image_rgb_avg_r  image_rgb_avg_g  \\\n","33698                           light         0.552707         0.519490   \n","33699                            dark         0.405319         0.362328   \n","33700                            dark         0.405319         0.362328   \n","33701                            dark         0.343913         0.316154   \n","33702                            dark         0.343913         0.316154   \n","33703                            dark         0.136002         0.124840   \n","33704                            dark         0.136002         0.124840   \n","\n","       image_rgb_avg_b  image_hsv_avg_h  image_hsv_avg_s  image_hsv_avg_v  \\\n","33698         0.512211         0.254606         0.208871         0.568104   \n","33699         0.354209         0.183127         0.219872         0.419370   \n","33700         0.354209         0.183127         0.219872         0.419370   \n","33701         0.307026         0.338734         0.255500         0.356855   \n","33702         0.307026         0.338734         0.255500         0.356855   \n","33703         0.121187         0.313693         0.260449         0.140653   \n","33704         0.121187         0.313693         0.260449         0.140653   \n","\n","       object_brightness_value object_brightness_interpretation  \\\n","33698               219.471943                            light   \n","33699               198.343621                            light   \n","33700               175.065043                            light   \n","33701               151.929901                            light   \n","33702               137.296597                            light   \n","33703                60.404664                             dark   \n","33704                54.514213                             dark   \n","\n","       object_rgb_avg_r  object_rgb_avg_g  object_rgb_avg_b  object_hsv_avg_h  \\\n","33698          0.900724          0.844611          0.839133          0.215730   \n","33699          0.790550          0.769863          0.786081          0.294754   \n","33700          0.755075          0.657424          0.657277          0.318293   \n","33701          0.607832          0.591539          0.586746          0.379261   \n","33702          0.574613          0.523310          0.521784          0.416595   \n","33703          0.241760          0.235164          0.233139          0.361929   \n","33704          0.228268          0.207731          0.207138          0.402688   \n","\n","       object_hsv_avg_s  object_hsv_avg_v  detected  confidence  object_size  \\\n","33698          0.116299          0.918348     False    0.000000      10486.0   \n","33699          0.135345          0.818334     False    0.000000       2240.0   \n","33700          0.271466          0.785582      True    0.912399      10486.0   \n","33701          0.131465          0.630860     False    0.000000       2240.0   \n","33702          0.209666          0.603817      True    0.821931      10486.0   \n","33703          0.113127          0.249445     False    0.000000       2240.0   \n","33704          0.190001          0.238191     False    0.000000      10486.0   \n","\n","      object_class  \n","33698          Bus  \n","33699          Bus  \n","33700          Bus  \n","33701          Bus  \n","33702          Bus  \n","33703          Bus  \n","33704          Bus  "],"text/html":["\n","  <div id=\"df-9ec02fe4-0228-4a34-be14-867f62b3825d\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>image_id</th>\n","      <th>image_version</th>\n","      <th>image_brightness_value</th>\n","      <th>image_brightness_interpretation</th>\n","      <th>image_rgb_avg_r</th>\n","      <th>image_rgb_avg_g</th>\n","      <th>image_rgb_avg_b</th>\n","      <th>image_hsv_avg_h</th>\n","      <th>image_hsv_avg_s</th>\n","      <th>image_hsv_avg_v</th>\n","      <th>object_brightness_value</th>\n","      <th>object_brightness_interpretation</th>\n","      <th>object_rgb_avg_r</th>\n","      <th>object_rgb_avg_g</th>\n","      <th>object_rgb_avg_b</th>\n","      <th>object_hsv_avg_h</th>\n","      <th>object_hsv_avg_s</th>\n","      <th>object_hsv_avg_v</th>\n","      <th>detected</th>\n","      <th>confidence</th>\n","      <th>object_size</th>\n","      <th>object_class</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>33698</th>\n","      <td>3044</td>\n","      <td>bright</td>\n","      <td>134.776908</td>\n","      <td>light</td>\n","      <td>0.552707</td>\n","      <td>0.519490</td>\n","      <td>0.512211</td>\n","      <td>0.254606</td>\n","      <td>0.208871</td>\n","      <td>0.568104</td>\n","      <td>219.471943</td>\n","      <td>light</td>\n","      <td>0.900724</td>\n","      <td>0.844611</td>\n","      <td>0.839133</td>\n","      <td>0.215730</td>\n","      <td>0.116299</td>\n","      <td>0.918348</td>\n","      <td>False</td>\n","      <td>0.000000</td>\n","      <td>10486.0</td>\n","      <td>Bus</td>\n","    </tr>\n","    <tr>\n","      <th>33699</th>\n","      <td>3044</td>\n","      <td>contrast</td>\n","      <td>95.425101</td>\n","      <td>dark</td>\n","      <td>0.405319</td>\n","      <td>0.362328</td>\n","      <td>0.354209</td>\n","      <td>0.183127</td>\n","      <td>0.219872</td>\n","      <td>0.419370</td>\n","      <td>198.343621</td>\n","      <td>light</td>\n","      <td>0.790550</td>\n","      <td>0.769863</td>\n","      <td>0.786081</td>\n","      <td>0.294754</td>\n","      <td>0.135345</td>\n","      <td>0.818334</td>\n","      <td>False</td>\n","      <td>0.000000</td>\n","      <td>2240.0</td>\n","      <td>Bus</td>\n","    </tr>\n","    <tr>\n","      <th>33700</th>\n","      <td>3044</td>\n","      <td>contrast</td>\n","      <td>95.425101</td>\n","      <td>dark</td>\n","      <td>0.405319</td>\n","      <td>0.362328</td>\n","      <td>0.354209</td>\n","      <td>0.183127</td>\n","      <td>0.219872</td>\n","      <td>0.419370</td>\n","      <td>175.065043</td>\n","      <td>light</td>\n","      <td>0.755075</td>\n","      <td>0.657424</td>\n","      <td>0.657277</td>\n","      <td>0.318293</td>\n","      <td>0.271466</td>\n","      <td>0.785582</td>\n","      <td>True</td>\n","      <td>0.912399</td>\n","      <td>10486.0</td>\n","      <td>Bus</td>\n","    </tr>\n","    <tr>\n","      <th>33701</th>\n","      <td>3044</td>\n","      <td>sharp</td>\n","      <td>82.461668</td>\n","      <td>dark</td>\n","      <td>0.343913</td>\n","      <td>0.316154</td>\n","      <td>0.307026</td>\n","      <td>0.338734</td>\n","      <td>0.255500</td>\n","      <td>0.356855</td>\n","      <td>151.929901</td>\n","      <td>light</td>\n","      <td>0.607832</td>\n","      <td>0.591539</td>\n","      <td>0.586746</td>\n","      <td>0.379261</td>\n","      <td>0.131465</td>\n","      <td>0.630860</td>\n","      <td>False</td>\n","      <td>0.000000</td>\n","      <td>2240.0</td>\n","      <td>Bus</td>\n","    </tr>\n","    <tr>\n","      <th>33702</th>\n","      <td>3044</td>\n","      <td>sharp</td>\n","      <td>82.461668</td>\n","      <td>dark</td>\n","      <td>0.343913</td>\n","      <td>0.316154</td>\n","      <td>0.307026</td>\n","      <td>0.338734</td>\n","      <td>0.255500</td>\n","      <td>0.356855</td>\n","      <td>137.296597</td>\n","      <td>light</td>\n","      <td>0.574613</td>\n","      <td>0.523310</td>\n","      <td>0.521784</td>\n","      <td>0.416595</td>\n","      <td>0.209666</td>\n","      <td>0.603817</td>\n","      <td>True</td>\n","      <td>0.821931</td>\n","      <td>10486.0</td>\n","      <td>Bus</td>\n","    </tr>\n","    <tr>\n","      <th>33703</th>\n","      <td>3044</td>\n","      <td>dark</td>\n","      <td>32.575656</td>\n","      <td>dark</td>\n","      <td>0.136002</td>\n","      <td>0.124840</td>\n","      <td>0.121187</td>\n","      <td>0.313693</td>\n","      <td>0.260449</td>\n","      <td>0.140653</td>\n","      <td>60.404664</td>\n","      <td>dark</td>\n","      <td>0.241760</td>\n","      <td>0.235164</td>\n","      <td>0.233139</td>\n","      <td>0.361929</td>\n","      <td>0.113127</td>\n","      <td>0.249445</td>\n","      <td>False</td>\n","      <td>0.000000</td>\n","      <td>2240.0</td>\n","      <td>Bus</td>\n","    </tr>\n","    <tr>\n","      <th>33704</th>\n","      <td>3044</td>\n","      <td>dark</td>\n","      <td>32.575656</td>\n","      <td>dark</td>\n","      <td>0.136002</td>\n","      <td>0.124840</td>\n","      <td>0.121187</td>\n","      <td>0.313693</td>\n","      <td>0.260449</td>\n","      <td>0.140653</td>\n","      <td>54.514213</td>\n","      <td>dark</td>\n","      <td>0.228268</td>\n","      <td>0.207731</td>\n","      <td>0.207138</td>\n","      <td>0.402688</td>\n","      <td>0.190001</td>\n","      <td>0.238191</td>\n","      <td>False</td>\n","      <td>0.000000</td>\n","      <td>10486.0</td>\n","      <td>Bus</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9ec02fe4-0228-4a34-be14-867f62b3825d')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-9ec02fe4-0228-4a34-be14-867f62b3825d button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-9ec02fe4-0228-4a34-be14-867f62b3825d');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":81}],"source":["df = pd.DataFrame.from_dict(metadata)\n","df.to_excel(join(OUTPUT_REPORTS, output_testing_report))\n","df.tail(7)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"Testing_FasterRCNN_ResNet50.ipynb","provenance":[],"authorship_tag":"ABX9TyNB3MwsW0yOBk0GCw0AUS/D"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"febdb11b259b4933b8ebb8e49f7ef374":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_06154f4ccd18411cbfbe1c2384ec4d30","IPY_MODEL_cb33a300719c401192ef3976ef6ce06d","IPY_MODEL_8568257107ce45e9bcbf8e43a9e1f681"],"layout":"IPY_MODEL_6a7af6aef63f4ba8a3de07432490ee47"}},"06154f4ccd18411cbfbe1c2384ec4d30":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f9112828294d495d8a36117cd45038b8","placeholder":"​","style":"IPY_MODEL_d0a15509f8904e209c0f8c7b3bc09532","value":"100%"}},"cb33a300719c401192ef3976ef6ce06d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_61494534609946c986349f39860dead2","max":102530333,"min":0,"orientation":"horizontal","style":"IPY_MODEL_69910f928a7442dc9e2cd8ef2a752dc7","value":102530333}},"8568257107ce45e9bcbf8e43a9e1f681":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c2492054e9f042709d93116fe58a51d9","placeholder":"​","style":"IPY_MODEL_268977162a114b21ab2d46ad0ff0ed64","value":" 97.8M/97.8M [00:00&lt;00:00, 190MB/s]"}},"6a7af6aef63f4ba8a3de07432490ee47":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f9112828294d495d8a36117cd45038b8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d0a15509f8904e209c0f8c7b3bc09532":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"61494534609946c986349f39860dead2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"69910f928a7442dc9e2cd8ef2a752dc7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c2492054e9f042709d93116fe58a51d9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"268977162a114b21ab2d46ad0ff0ed64":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}