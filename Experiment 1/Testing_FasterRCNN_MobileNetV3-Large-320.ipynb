{"cells":[{"cell_type":"markdown","metadata":{"id":"Ymf05TlF5QAU"},"source":["# Main Configs"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"zHVD0it245SC","executionInfo":{"status":"ok","timestamp":1650288432838,"user_tz":-120,"elapsed":4,"user":{"displayName":"Bastian B.","userId":"10033977703569380654"}}},"outputs":[],"source":["small_df = False\n","batch_size = 1\n","width, height = 300, 300"]},{"cell_type":"markdown","metadata":{"id":"4BC8oFZjyN_v"},"source":["# Setup and Imports"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25232,"status":"ok","timestamp":1650288458067,"user":{"displayName":"Bastian B.","userId":"10033977703569380654"},"user_tz":-120},"id":"L07xyJhpDajY","outputId":"941ec6c6-aeec-4cda-949a-632780d26139"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"e_ov9EndF-XS","executionInfo":{"status":"ok","timestamp":1650288483593,"user_tz":-120,"elapsed":25530,"user":{"displayName":"Bastian B.","userId":"10033977703569380654"}}},"outputs":[],"source":["!mkdir /usr/lib/python3.7/metrics\n","!cp -R /content/drive/MyDrive/BA/Notebooks/2_Experiment/review_object_detection_metrics-main/src /usr/lib/python3.7/metrics/src"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"wo1xjbFjNOvW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650288500153,"user_tz":-120,"elapsed":16564,"user":{"displayName":"Bastian B.","userId":"10033977703569380654"}},"outputId":"80dc9ac9-6326-4975-d2fe-1136f16d02e5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting PyQt5\n","  Downloading PyQt5-5.15.6-cp36-abi3-manylinux1_x86_64.whl (8.3 MB)\n","\u001b[K     |████████████████████████████████| 8.3 MB 22.7 MB/s \n","\u001b[?25hCollecting PyQt5-sip<13,>=12.8\n","  Downloading PyQt5_sip-12.10.1-cp37-cp37m-manylinux1_x86_64.whl (338 kB)\n","\u001b[K     |████████████████████████████████| 338 kB 65.4 MB/s \n","\u001b[?25hCollecting PyQt5-Qt5>=5.15.2\n","  Downloading PyQt5_Qt5-5.15.2-py3-none-manylinux2014_x86_64.whl (59.9 MB)\n","\u001b[K     |████████████████████████████████| 59.9 MB 145.5 MB/s \n","\u001b[?25hInstalling collected packages: PyQt5-sip, PyQt5-Qt5, PyQt5\n","Successfully installed PyQt5-5.15.6 PyQt5-Qt5-5.15.2 PyQt5-sip-12.10.1\n","\u001b[K     |████████████████████████████████| 48 kB 5.6 MB/s \n","\u001b[K     |████████████████████████████████| 948 kB 51.4 MB/s \n","\u001b[K     |████████████████████████████████| 78 kB 8.8 MB/s \n","\u001b[K     |████████████████████████████████| 60 kB 8.6 MB/s \n","\u001b[K     |████████████████████████████████| 229 kB 86.0 MB/s \n","\u001b[K     |████████████████████████████████| 58 kB 7.2 MB/s \n","\u001b[K     |████████████████████████████████| 10.9 MB 83.8 MB/s \n","\u001b[K     |████████████████████████████████| 51 kB 7.8 MB/s \n","\u001b[?25h  Building wheel for typing (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.4.0 which is incompatible.\u001b[0m\n"]}],"source":["!pip install PyQt5\n","!pip install -qU torch_snippets"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"XJigMscbLLYm","executionInfo":{"status":"ok","timestamp":1650288508691,"user_tz":-120,"elapsed":8547,"user":{"displayName":"Bastian B.","userId":"10033977703569380654"}}},"outputs":[],"source":["import copy\n","import glob\n","import torch\n","import time\n","import statistics\n","import cv2\n","import pandas as pd\n","import IPython\n","\n","from os.path import join\n","from torch_snippets import *\n","from PIL import Image\n","from metrics.src.evaluators import coco_evaluator, pascal_voc_evaluator\n","from metrics.src.bounding_box import BoundingBox\n","from metrics.src.utils.enumerators import BBFormat, BBType, CoordinatesType, MethodAveragePrecision\n","from skimage import data\n","from skimage.color import rgb2hsv, rgb2luv, rgb2lab\n","from skimage.util import random_noise"]},{"cell_type":"code","source":["pd.set_option('display.max_columns', None)"],"metadata":{"id":"v68G-1jvOrNR","executionInfo":{"status":"ok","timestamp":1650288508692,"user_tz":-120,"elapsed":15,"user":{"displayName":"Bastian B.","userId":"10033977703569380654"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","execution_count":7,"metadata":{"id":"tbSWaqcrXVJA","executionInfo":{"status":"ok","timestamp":1650288508693,"user_tz":-120,"elapsed":13,"user":{"displayName":"Bastian B.","userId":"10033977703569380654"}}},"outputs":[],"source":["IMAGE_ROOT = '/content/drive/MyDrive/BA/dataset/bus-trucks/images'\n","OUTPUT_REPORTS = '/content/drive/MyDrive/BA/Notebooks/2_Experiment/Output_Reports'\n","OUTPUT_MODELS = '/content/drive/MyDrive/BA/Notebooks/2_Experiment/Output_Models'\n","output_testing_report = 'testing_faster_rcnn_mobilenetv3_large_320.xlsx'\n","output_training_report = 'faster_rcnn_mobilenetv3_large_320.csv'\n","output_model_name = 'faster_rcnn_mobilenetv3_large_320.pt'"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":1677,"status":"ok","timestamp":1650288510358,"user":{"displayName":"Bastian B.","userId":"10033977703569380654"},"user_tz":-120},"id":"8K9USH1CGFjf","outputId":"b3e9f869-32bc-42a0-dd98-558368248018"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["               filename  class      xmin      xmax      ymin      ymax\n","0  00013f14dd4e168f.jpg    Bus  0.287500  0.999375  0.194184  0.999062\n","1  0002914fa805e227.jpg  Truck  0.061250  0.966875  0.125399  0.974495\n","2  0005f203463a13a8.jpg  Truck  0.000000  0.700000  0.187778  0.998889\n","3  00066517f9d814f9.jpg  Truck  0.000000  0.588867  0.069892  0.998208\n","4  000812dcf304a8e7.jpg    Bus  0.059375  0.848750  0.029936  0.958660"],"text/html":["\n","  <div id=\"df-9977488f-1567-40fb-aedd-c9ef49105829\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>filename</th>\n","      <th>class</th>\n","      <th>xmin</th>\n","      <th>xmax</th>\n","      <th>ymin</th>\n","      <th>ymax</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>00013f14dd4e168f.jpg</td>\n","      <td>Bus</td>\n","      <td>0.287500</td>\n","      <td>0.999375</td>\n","      <td>0.194184</td>\n","      <td>0.999062</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0002914fa805e227.jpg</td>\n","      <td>Truck</td>\n","      <td>0.061250</td>\n","      <td>0.966875</td>\n","      <td>0.125399</td>\n","      <td>0.974495</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0005f203463a13a8.jpg</td>\n","      <td>Truck</td>\n","      <td>0.000000</td>\n","      <td>0.700000</td>\n","      <td>0.187778</td>\n","      <td>0.998889</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>00066517f9d814f9.jpg</td>\n","      <td>Truck</td>\n","      <td>0.000000</td>\n","      <td>0.588867</td>\n","      <td>0.069892</td>\n","      <td>0.998208</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>000812dcf304a8e7.jpg</td>\n","      <td>Bus</td>\n","      <td>0.059375</td>\n","      <td>0.848750</td>\n","      <td>0.029936</td>\n","      <td>0.958660</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9977488f-1567-40fb-aedd-c9ef49105829')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-9977488f-1567-40fb-aedd-c9ef49105829 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-9977488f-1567-40fb-aedd-c9ef49105829');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":8}],"source":["df_train = pd.read_csv('/content/drive/MyDrive/BA/dataset/Experimente/df_80_20_train_tf.csv')\n","df_test = pd.read_csv('/content/drive/MyDrive/BA/dataset/Experimente/df_80_20_test_tf.csv')\n","df_test.head()"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1650288510359,"user":{"displayName":"Bastian B.","userId":"10033977703569380654"},"user_tz":-120},"id":"2wq9qa5eLQ1A","outputId":"bc52d668-e58d-4023-df50-c66f035f5cce"},"outputs":[{"output_type":"stream","name":"stdout","text":["{1: 'Bus', 2: 'Truck', 0: 'background'}\n"]}],"source":["label2target = {l:t+1 for t,l in enumerate(df_train['class'].unique())}\n","label2target['background'] = 0\n","target2label = {t:l for l,t in label2target.items()}\n","label2target = {v: k for k, v in target2label.items()}\n","background_class = label2target['background']\n","num_classes = len(label2target)\n","\n","print(target2label)"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":835,"status":"ok","timestamp":1650288511190,"user":{"displayName":"Bastian B.","userId":"10033977703569380654"},"user_tz":-120},"id":"oR_KeMMcCq8j","outputId":"77666f76-dd54-4441-c225-2e6d6bfc3c46"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["   epoch      time      loss  loss_classifier  loss_box_reg  loss_objectness  \\\n","0      0  0.532456  1.826923         1.363694      0.373133         0.085319   \n","1      0  0.114823  1.570877         1.115933      0.405240         0.043967   \n","2      0  0.120514  0.967652         0.608759      0.279076         0.075146   \n","3      0  0.105697  0.982080         0.448771      0.496579         0.033435   \n","4      0  0.112777  0.755007         0.352983      0.363649         0.035939   \n","\n","   loss_rpn_box_reg  \n","0          0.004777  \n","1          0.005737  \n","2          0.004671  \n","3          0.003295  \n","4          0.002436  "],"text/html":["\n","  <div id=\"df-46e6b6a3-19ad-4631-8a75-c62acdf1487e\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>epoch</th>\n","      <th>time</th>\n","      <th>loss</th>\n","      <th>loss_classifier</th>\n","      <th>loss_box_reg</th>\n","      <th>loss_objectness</th>\n","      <th>loss_rpn_box_reg</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0.532456</td>\n","      <td>1.826923</td>\n","      <td>1.363694</td>\n","      <td>0.373133</td>\n","      <td>0.085319</td>\n","      <td>0.004777</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>0.114823</td>\n","      <td>1.570877</td>\n","      <td>1.115933</td>\n","      <td>0.405240</td>\n","      <td>0.043967</td>\n","      <td>0.005737</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>0.120514</td>\n","      <td>0.967652</td>\n","      <td>0.608759</td>\n","      <td>0.279076</td>\n","      <td>0.075146</td>\n","      <td>0.004671</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>0.105697</td>\n","      <td>0.982080</td>\n","      <td>0.448771</td>\n","      <td>0.496579</td>\n","      <td>0.033435</td>\n","      <td>0.003295</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>0.112777</td>\n","      <td>0.755007</td>\n","      <td>0.352983</td>\n","      <td>0.363649</td>\n","      <td>0.035939</td>\n","      <td>0.002436</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-46e6b6a3-19ad-4631-8a75-c62acdf1487e')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-46e6b6a3-19ad-4631-8a75-c62acdf1487e button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-46e6b6a3-19ad-4631-8a75-c62acdf1487e');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":10}],"source":["df_report = pd.read_csv(join(OUTPUT_REPORTS, output_training_report))\n","df_report.head()"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"Q2ryFbUNs85q","executionInfo":{"status":"ok","timestamp":1650288511191,"user_tz":-120,"elapsed":4,"user":{"displayName":"Bastian B.","userId":"10033977703569380654"}}},"outputs":[],"source":["def preprocess_image(img):\n","  img = torch.tensor(img).permute(2,0,1)\n","  return img.to(device).float()\n","\n","class OpenDataset(torch.utils.data.Dataset):\n","  w, h = width, height\n","  def __init__(self, df, image_dir=IMAGE_ROOT):\n","    self.image_dir = image_dir\n","    self.files = glob.glob(self.image_dir+'/*')\n","    self.df = df\n","    self.image_infos = df['filename'].unique()\n","\n","  def __getitem__(self, ix):\n","\n","    #filename\tclass\txmin\txmax\tymin\tymax\n","\n","    # # load images and masks\n","    image_id = self.image_infos[ix]\n","    img_path = find(image_id, self.files)\n","    img = Image.open(img_path).convert('RGB')\n","    img = np.array(img.resize((self.w, self.h), resample=Image.BILINEAR))/255.\n","\n","    data = self.df[self.df['filename'] == image_id]\n","    labels = data['class'].values.tolist()\n","    data = data[['xmin','ymin','xmax','ymax']].values\n","    data[:,[0,2]] *= self.w\n","    data[:,[1,3]] *= self.h\n","    boxes = data.astype(np.uint32).tolist() # convert to absolute coordinates\n","    # torch FRCNN expects ground truths as a dictionary of tensors\n","    target = {}\n","    target[\"boxes\"] = torch.Tensor(boxes).float()\n","    target[\"labels\"] = torch.Tensor([label2target[i] for i in labels]).long()\n","    img = preprocess_image(img)\n","\n","    return img, target\n","  \n","  @classmethod\n","  def get_image_meta(self, ix):\n","    image_id = self.image_infos[ix]\n","    img_path = find(image_id, self.files)\n","    img = Image.open(img_path).convert('RGB')\n","    img = np.array(img)/255.\n","\n","    return image_id, img.shape\n","\n","  def collate_fn(self, batch):\n","    return tuple(zip(*batch)) \n","\n","  def __len__(self):\n","    return len(self.image_infos)"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"N_7ROj0gwIU1","executionInfo":{"status":"ok","timestamp":1650288511191,"user_tz":-120,"elapsed":4,"user":{"displayName":"Bastian B.","userId":"10033977703569380654"}}},"outputs":[],"source":["if small_df:\n","  df_test = df_test[:50]"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"cHEx-uaWP_5G","executionInfo":{"status":"ok","timestamp":1650288627175,"user_tz":-120,"elapsed":115987,"user":{"displayName":"Bastian B.","userId":"10033977703569380654"}}},"outputs":[],"source":["test_ds = OpenDataset(df_test)\n","test_loader = DataLoader(test_ds, batch_size=batch_size, collate_fn=test_ds.collate_fn, drop_last=True)"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"tGiY3lyIFA4C","colab":{"base_uri":"https://localhost:8080/","height":66,"referenced_widgets":["7892fbb940404d9286a17221d84578a3","e4aeb3050baf474688c0b6760a2a3418","58ddc9790e57486dadaba01c4d2b8182","b9ded4a8dc9240ba934fafc83710f673","09dc7f97c0ed47448fdb5e7c4f5f208a","2b0cad322f6840f7a6b496acbcd20e79","89732667f6c24fe285c915af1bec620f","b7e8a5f728474e1a9bbaa247b6ed4274","bcf029e87c644e8a882fa22f522e98af","a082ade322c948dd90e5e10dd781e07c","625efa976af84705a271e2544a34fb60"]},"executionInfo":{"status":"ok","timestamp":1650288640880,"user_tz":-120,"elapsed":13709,"user":{"displayName":"Bastian B.","userId":"10033977703569380654"}},"outputId":"972a8c0a-1145-4f66-8830-123b07895a5f"},"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/mobilenet_v3_large-8738ca79.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v3_large-8738ca79.pth\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0.00/21.1M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7892fbb940404d9286a17221d84578a3"}},"metadata":{}}],"source":["import torchvision\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","\n","PATH = join(OUTPUT_MODELS, output_model_name)\n","\n","# Load\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_320_fpn(pretrained=False)\n","in_features = model.roi_heads.box_predictor.cls_score.in_features\n","model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","model.load_state_dict(torch.load(PATH, map_location=device))\n","model.to(device);"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"Y7N1vClMVUx1","executionInfo":{"status":"ok","timestamp":1650288640881,"user_tz":-120,"elapsed":3,"user":{"displayName":"Bastian B.","userId":"10033977703569380654"}}},"outputs":[],"source":["from torchvision.ops import nms\n","def decode_output(output):\n","  'convert tensors to numpy arrays'\n","  bbs = output['boxes'].cpu().detach().numpy().astype(np.uint16)\n","  labels = np.array([target2label[i] for i in output['labels'].cpu().detach().numpy()])\n","  confs = output['scores'].cpu().detach().numpy()\n","  ixs = nms(torch.tensor(bbs.astype(np.float32)), torch.tensor(confs), 0.05)\n","  bbs, confs, labels = [tensor[ixs] for tensor in [bbs, confs, labels]]\n","\n","  if len(ixs) == 1:\n","      bbs, confs, labels = [np.array([tensor]) for tensor in [bbs, confs, labels]]\n","  return bbs.tolist(), confs.tolist(), labels.tolist()"]},{"cell_type":"markdown","metadata":{"id":"XCVKW0B_YKLN"},"source":["# Size Analysis"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2758944,"status":"ok","timestamp":1650291399823,"user":{"displayName":"Bastian B.","userId":"10033977703569380654"},"user_tz":-120},"id":"mVtlcTIPVaee","outputId":"179e2e40-2510-4a0e-e89c-3ead59d4d954"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  \n","/usr/local/lib/python3.7/dist-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n","  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"]}],"source":["bounding_boxes_detected = []\n","bounding_boxes_gt = []\n","model.eval()\n","\n","for ix, (images, targets) in enumerate(test_loader):\n","  images = [torch.tensor(im).type(torch.FloatTensor).to(device) for im in images]\n","  outputs = model(images)\n","  outputs = decode_output(outputs[0])\n","\n","  #Detected\n","  for i, output in enumerate(outputs[0]):\n","    bb = BoundingBox(\n","      image_name        = str(ix),\n","      class_id          = outputs[2][i],\n","      coordinates       = output,\n","      type_coordinates  = CoordinatesType.ABSOLUTE,\n","      bb_type           = BBType.DETECTED,\n","      confidence        = outputs[1][i],\n","      format            = BBFormat.XYX2Y2\n","    )\n","    bounding_boxes_detected.append(bb)\n","\n","\n","  #Ground Truth\n","  for i, box in enumerate(targets[0]['boxes']):\n","    cls = target2label[int(targets[0]['labels'][i])]\n","    bb = BoundingBox(\n","      image_name        = str(ix),\n","      class_id          = cls,\n","      coordinates       = list(box),\n","      type_coordinates  = CoordinatesType.ABSOLUTE,\n","      bb_type           = BBType.GROUND_TRUTH,\n","      confidence        = None,\n","      format            = BBFormat.XYX2Y2\n","    )\n","    bounding_boxes_gt.append(bb)"]},{"cell_type":"markdown","metadata":{"id":"oOW8t1uOU9UG"},"source":[""]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7397,"status":"ok","timestamp":1650291407218,"user":{"displayName":"Bastian B.","userId":"10033977703569380654"},"user_tz":-120},"id":"-Cafjb_keoRd","outputId":"7be56b9c-de2e-41d0-8c47-a287bea104cb"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'AP': 0.4290140895847271,\n"," 'AP50': 0.6093659323250014,\n"," 'AP75': 0.48446757313110056,\n"," 'APlarge': 0.7152754014528375,\n"," 'APmedium': 0.6618762098248865,\n"," 'APsmall': 0.23745070073719793,\n"," 'AR1': 0.43372733719639084,\n"," 'AR10': 0.4806976208916135,\n"," 'AR100': 0.4806976208916135,\n"," 'ARlarge': 0.771267931162964,\n"," 'ARmedium': 0.7208207642031171,\n"," 'ARsmall': 0.28920182032803177}"]},"metadata":{},"execution_count":17}],"source":["coco_evaluator.get_coco_summary(\n","    groundtruth_bbs = bounding_boxes_gt,\n","    detected_bbs = bounding_boxes_detected,\n","    small_size = 146,\n","    medium_size = 228\n",")"]},{"cell_type":"markdown","metadata":{"id":"-XplV_SmU4a1"},"source":["# Full COCO"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1650291407218,"user":{"displayName":"Bastian B.","userId":"10033977703569380654"},"user_tz":-120},"id":"jMpnwblAvg0r","outputId":"26b6fff7-cda5-498d-d9c1-da537baa37f4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                                      Bus  \\\n","class                                                                 Bus   \n","precision               [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...   \n","recall                  [0.00043233895373973193, 0.0008646779074794639...   \n","AP                                                               0.636113   \n","interpolated precision  [1.0, 1.0, 1.0, 1.0, 0.9825581395348837, 0.982...   \n","\n","                                                                    Truck  \n","class                                                               Truck  \n","precision               [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...  \n","recall                  [0.0003996802557953637, 0.0007993605115907274,...  \n","AP                                                               0.582619  \n","interpolated precision  [1.0, 1.0, 1.0, 1.0, 0.9935897435897436, 0.993...  "],"text/html":["\n","  <div id=\"df-fddf2679-edb4-4fbb-a475-9930a65c4a3b\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Bus</th>\n","      <th>Truck</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>class</th>\n","      <td>Bus</td>\n","      <td>Truck</td>\n","    </tr>\n","    <tr>\n","      <th>precision</th>\n","      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n","      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>recall</th>\n","      <td>[0.00043233895373973193, 0.0008646779074794639...</td>\n","      <td>[0.0003996802557953637, 0.0007993605115907274,...</td>\n","    </tr>\n","    <tr>\n","      <th>AP</th>\n","      <td>0.636113</td>\n","      <td>0.582619</td>\n","    </tr>\n","    <tr>\n","      <th>interpolated precision</th>\n","      <td>[1.0, 1.0, 1.0, 1.0, 0.9825581395348837, 0.982...</td>\n","      <td>[1.0, 1.0, 1.0, 1.0, 0.9935897435897436, 0.993...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fddf2679-edb4-4fbb-a475-9930a65c4a3b')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-fddf2679-edb4-4fbb-a475-9930a65c4a3b button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-fddf2679-edb4-4fbb-a475-9930a65c4a3b');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":18}],"source":["df = coco_evaluator.get_coco_metrics(\n","        bounding_boxes_gt,\n","        bounding_boxes_detected,\n","        iou_threshold=0.5,\n","        area_range=(0, np.inf),\n","        max_dets=100,\n",")\n","df = pd.DataFrame.from_dict(df)\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"TOVY3AZhU1f0"},"source":["# PASCAL VOL"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":112},"executionInfo":{"elapsed":1939,"status":"ok","timestamp":1650291409155,"user":{"displayName":"Bastian B.","userId":"10033977703569380654"},"user_tz":-120},"id":"umuNe92_sDXL","outputId":"dd0a7913-3d7c-47a2-d456-8048f44cb68e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                               per_class       mAP\n","Bus    {'precision': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1...  0.673317\n","Truck  {'precision': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1...  0.673317"],"text/html":["\n","  <div id=\"df-54483486-831e-4777-8488-bfca37e5013c\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>per_class</th>\n","      <th>mAP</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Bus</th>\n","      <td>{'precision': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1...</td>\n","      <td>0.673317</td>\n","    </tr>\n","    <tr>\n","      <th>Truck</th>\n","      <td>{'precision': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1...</td>\n","      <td>0.673317</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-54483486-831e-4777-8488-bfca37e5013c')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-54483486-831e-4777-8488-bfca37e5013c button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-54483486-831e-4777-8488-bfca37e5013c');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":19}],"source":["df = pascal_voc_evaluator.get_pascalvoc_metrics(\n","    bounding_boxes_gt,\n","    bounding_boxes_detected,\n","    iou_threshold= 0.1,\n","    method=MethodAveragePrecision.EVERY_POINT_INTERPOLATION\n",")\n","\n","df = pd.DataFrame.from_dict(df)\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"RnM0quJQYHJp"},"source":["# Color Analysis"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"bTgNQN5mVRpP","executionInfo":{"status":"ok","timestamp":1650291409155,"user_tz":-120,"elapsed":9,"user":{"displayName":"Bastian B.","userId":"10033977703569380654"}}},"outputs":[],"source":["def image_light(img, thrshld = 127):\n","  if type(img) is torch.Tensor:\n","    img = img.cpu().permute(1,2,0).numpy()\n","  rgb = img * 255\n","  rgb = np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140])\n","  \n","  is_light = np.mean(rgb) > thrshld\n","\n","  # Light: 0\n","  # Dark: 1\n","\n","  # return 0 if is_light else 1\n","  return {'value': np.mean(rgb),\n","          'brightness': 'light' if is_light else 'dark'\n","          }\n","\n","def avg_img_color(img):\n","  if type(img) is torch.Tensor:\n","    img = img.cpu().permute(1,2,0).numpy()\n","  rgb_avg = img.mean(axis=(0, 1)) \n","  hsv_avg = rgb2hsv(img).mean(axis=(0, 1))\n","  luv_avg = rgb2luv(img).mean(axis=(0, 1))\n","  lab_avg = rgb2lab(img).mean(axis=(0, 1))\n","\n","  return {\n","    'rgb_avg_r': float(rgb_avg[0]),\n","    'rgb_avg_g': float(rgb_avg[1]),\n","    'rgb_avg_b': float(rgb_avg[2]),\n","    'hsv_avg_h': float(hsv_avg[0]),\n","    'hsv_avg_s': float(hsv_avg[1]),\n","    'hsv_avg_v': float(hsv_avg[2])\n","  }\n","\n","def get_sharpness(img):\n","  if type(img) is torch.Tensor:\n","    img = img.cpu().permute(1,2,0).numpy()\n","  i = np.dot(img[...,:3], [0.2989, 0.5870, 0.1140])\n","\n","  gy, gx = np.gradient(i)\n","  gnorm = np.sqrt(gx**2 + gy**2)\n","  sharpness = np.average(gnorm)\n","  return sharpness"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"Nl0iDKb_VRmH","executionInfo":{"status":"ok","timestamp":1650291410395,"user_tz":-120,"elapsed":8,"user":{"displayName":"Bastian B.","userId":"10033977703569380654"}}},"outputs":[],"source":["def bb_intersection_over_union(boxA, boxB):\n","\n","  xA = max(boxA[0], boxB[0])\n","  yA = max(boxA[1], boxB[1])\n","  xB = min(boxA[2], boxB[2])\n","  yB = min(boxA[3], boxB[3])\n","\n","  interArea = (xB - xA) * (yB - yA)\n","\n","  boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n","  boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n","\n","  iou = interArea / float(boxAArea + boxBArea - interArea)\n","\n","  return iou\n","\n","\n","def detected(preds, gt, iou_threshold ):\n","  det = False\n","  max_conf = 0\n","  for i, pred in enumerate(preds):\n","    if pred[0] == gt[0]:\n","      if pred[1] == gt[1]:\n","        iuo = bb_intersection_over_union(pred[-4:], gt[-4:])\n","        if iuo >= iou_threshold:\n","          det = True\n","          if pred[2] > max_conf:\n","            max_conf = pred[2]\n","\n","  return det, max_conf"]},{"cell_type":"code","source":["def printProgressBar (iteration, total, prefix = '', suffix = '', decimals = 1, length = 100, fill = '█', printEnd = \"\\r\"):\n","    percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n","    filledLength = int(length * iteration // total)\n","    bar = fill * filledLength + '-' * (length - filledLength)\n","    string = f'\\r{prefix} |{bar}| {percent}% {suffix}'\n","    out.update(IPython.display.Pretty(string))\n","    # Print New Line on Complete\n","    if iteration == total: \n","        print()"],"metadata":{"id":"LQ3z_YNVBUh5","executionInfo":{"status":"ok","timestamp":1650291410396,"user_tz":-120,"elapsed":8,"user":{"displayName":"Bastian B.","userId":"10033977703569380654"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":245},"executionInfo":{"elapsed":2585534,"status":"ok","timestamp":1650293995922,"user":{"displayName":"Bastian B.","userId":"10033977703569380654"},"user_tz":-120},"id":"EEXba0bVVRjQ","outputId":"cbe45d8a-1815-4fed-d75e-2fd0c616b39b"},"outputs":[{"output_type":"display_data","data":{"text/plain":["\rProgress: |██████████████████████████████████████████████████| 100.0% Complete"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:164: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:236: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:314: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:391: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:468: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: RuntimeWarning: divide by zero encountered in true_divide\n","  del sys.path[0]\n"]},{"output_type":"stream","name":"stdout","text":["\n","CPU times: user 51min 42s, sys: 43.5 s, total: 52min 26s\n","Wall time: 43min 5s\n"]}],"source":["%%time\n","\n","blur_filter = 1/9 *  torch.tensor([[1, 1, 1], [1, 1, 1], [1, 1, 1]])\n","metadata = []\n","n_items = len(test_loader)\n","out = display(IPython.display.Pretty('Starting'), display_id=True)\n","\n","model.eval()\n","\n","for ix, (images, targets) in enumerate(test_loader):\n","  # if ix == 10:\n","  #   break\n","  img = images[0]\n","  t = targets[0]['boxes']\n","  img_ = Image.fromarray(np.uint8(img.cpu().permute(1, 2, 0).numpy() * 255))\n","\n","\n","  # plain image\n","  c_img = image_light(img)\n","  c_avg_img = avg_img_color(img)\n","\n","  preds = []\n","  outputs = model(images)\n","  for i_output, output in enumerate(outputs):\n","    bbs, confs, labels = decode_output(output)\n","    for i_bb, bb in enumerate(bbs):\n","      X1, Y1, X2, Y2 = bb\n","      preds.append([ix, labels[i_bb], confs[i_bb], X1, Y1, X2, Y2])\n","\n","\n","  for i, box in enumerate(targets[0]['boxes']):\n","    cls = target2label[int(targets[0]['labels'][i])]\n","\n","    X1, Y1, X2, Y2 = box.numpy().astype(int)\n","\n","    # object frame\n","    obj_frame = img[:3, Y1:Y2, X1:X2]\n","\n","    # colors\n","    c_obj = image_light(obj_frame)\n","    c_avg_obj = avg_img_color(obj_frame)\n","\n","    # Area\n","    bb = BoundingBox(\n","      image_name        = str(ix),\n","      class_id          = cls,\n","      coordinates       = list(box),\n","      type_coordinates  = CoordinatesType.ABSOLUTE,\n","      bb_type           = BBType.GROUND_TRUTH,\n","      confidence        = None,\n","      format            = BBFormat.XYX2Y2\n","    )\n","    \n","    # detected\n","    det, conf = detected(\n","        preds,\n","        [ix, cls, 1, X1, Y1, X2, Y2],\n","        0.6\n","    )\n","    metadata.append({'image_id': ix,\n","                    'image_version': 'standard',\n","                    'image_brightness_value': c_img['value'],\n","                    'image_brightness_interpretation': c_img['brightness'],\n","                    'image_rgb_avg_r': c_avg_img['rgb_avg_r'],\n","                    'image_rgb_avg_g': c_avg_img['rgb_avg_g'],\n","                    'image_rgb_avg_b': c_avg_img['rgb_avg_b'],\n","                    'image_hsv_avg_h': c_avg_img['hsv_avg_h'],\n","                    'image_hsv_avg_s': c_avg_img['hsv_avg_s'],\n","                    'image_hsv_avg_v': c_avg_img['hsv_avg_v'],\n","                    'object_brightness_value': c_obj['value'],\n","                    'object_brightness_interpretation': c_obj['brightness'],\n","                    'object_rgb_avg_r': c_avg_obj['rgb_avg_r'],\n","                    'object_rgb_avg_g': c_avg_obj['rgb_avg_g'],\n","                    'object_rgb_avg_b': c_avg_obj['rgb_avg_b'],\n","                    'object_hsv_avg_h': c_avg_obj['hsv_avg_h'],\n","                    'object_hsv_avg_s': c_avg_obj['hsv_avg_s'],\n","                    'object_hsv_avg_v': c_avg_obj['hsv_avg_v'],\n","                    'detected': det,\n","                    'confidence': conf,\n","                    'object_size': bb.get_area(),\n","                    'object_class': cls\n","                   }\n","                  )\n","###########################################################################################\n","\n","  # blurred image\n","  blurred_img = cv2.filter2D(img.cpu().permute(1, 2, 0).numpy(), -1, kernel = blur_filter.numpy())\n","  c_img = image_light(blurred_img)\n","  c_avg_img = avg_img_color(blurred_img)\n","\n","  blurred_imgs = [torch.from_numpy(blurred_img).permute(2, 0, 1)]\n","  blurred_imgs = [torch.tensor(im).type(torch.FloatTensor).to(device) for im in blurred_imgs]\n","  blurred_img = blurred_imgs[0]\n","\n","  preds = []\n","  outputs = model(blurred_imgs) \n","\n","  for i_output, output in enumerate(outputs):\n","    bbs, confs, labels = decode_output(output)\n","    for i_bb, bb in enumerate(bbs):\n","      X1, Y1, X2, Y2 = bb\n","      preds.append([ix, labels[i_bb], confs[i_bb], X1, Y1, X2, Y2])\n","\n","\n","  for i, box in enumerate(targets[0]['boxes']):\n","    cls = target2label[int(targets[0]['labels'][i])]\n","\n","    X1, Y1, X2, Y2 = box.numpy().astype(int)\n","\n","    # object frame\n","    obj_frame = blurred_img[:3, Y1:Y2, X1:X2]\n","\n","    # colors\n","    c_obj = image_light(obj_frame)\n","    c_avg_obj = avg_img_color(obj_frame)\n","\n","    # Area\n","    bb = BoundingBox(\n","      image_name        = str(ix),\n","      class_id          = cls,\n","      coordinates       = list(box),\n","      type_coordinates  = CoordinatesType.ABSOLUTE,\n","      bb_type           = BBType.GROUND_TRUTH,\n","      confidence        = None,\n","      format            = BBFormat.XYX2Y2\n","    )\n","    \n","    # detected\n","    det, conf = detected(\n","        preds,\n","        [ix, cls, 1, X1, Y1, X2, Y2],\n","        0.6\n","    )\n","    metadata.append({'image_id': ix,\n","                    'image_version': 'blurred',\n","                    'image_brightness_value': c_img['value'],\n","                    'image_brightness_interpretation': c_img['brightness'],\n","                    'image_rgb_avg_r': c_avg_img['rgb_avg_r'],\n","                    'image_rgb_avg_g': c_avg_img['rgb_avg_g'],\n","                    'image_rgb_avg_b': c_avg_img['rgb_avg_b'],\n","                    'image_hsv_avg_h': c_avg_img['hsv_avg_h'],\n","                    'image_hsv_avg_s': c_avg_img['hsv_avg_s'],\n","                    'image_hsv_avg_v': c_avg_img['hsv_avg_v'],\n","                    'object_brightness_value': c_obj['value'],\n","                    'object_brightness_interpretation': c_obj['brightness'],\n","                    'object_rgb_avg_r': c_avg_obj['rgb_avg_r'],\n","                    'object_rgb_avg_g': c_avg_obj['rgb_avg_g'],\n","                    'object_rgb_avg_b': c_avg_obj['rgb_avg_b'],\n","                    'object_hsv_avg_h': c_avg_obj['hsv_avg_h'],\n","                    'object_hsv_avg_s': c_avg_obj['hsv_avg_s'],\n","                    'object_hsv_avg_v': c_avg_obj['hsv_avg_v'],\n","                    'detected': det,\n","                    'confidence': conf,\n","                    'object_size': bb.get_area(),\n","                    'object_class': cls\n","                   }\n","                  )\n","###########################################################################################\n","  # noisy image\n","  noise_img = random_noise(img.cpu().permute(1, 2, 0).numpy(), mode='s&p',amount=0.05)\n","  c_img = image_light(noise_img)\n","  c_avg_img = avg_img_color(noise_img)\n","\n","  noisy_imgs = [torch.from_numpy(noise_img).permute(2, 0, 1)]\n","  noisy_imgs = [torch.tensor(im).type(torch.FloatTensor).to(device) for im in noisy_imgs]\n","  noise_img = noisy_imgs[0]\n","\n","  preds = []\n","  outputs = model(noisy_imgs) \n","\n","  for i_output, output in enumerate(outputs):\n","    bbs, confs, labels = decode_output(output)\n","    for i_bb, bb in enumerate(bbs):\n","      X1, Y1, X2, Y2 = bb\n","      preds.append([ix, labels[i_bb], confs[i_bb], X1, Y1, X2, Y2])\n","\n","\n","  for i, box in enumerate(targets[0]['boxes']):\n","    cls = target2label[int(targets[0]['labels'][i])]\n","\n","    X1, Y1, X2, Y2 = box.numpy().astype(int)\n","\n","    # object frame\n","    obj_frame = noise_img[:3, Y1:Y2, X1:X2]\n","\n","    # colors\n","    c_obj = image_light(obj_frame)\n","    c_avg_obj = avg_img_color(obj_frame)\n","\n","    # Area\n","    bb = BoundingBox(\n","      image_name        = str(ix),\n","      class_id          = cls,\n","      coordinates       = list(box),\n","      type_coordinates  = CoordinatesType.ABSOLUTE,\n","      bb_type           = BBType.GROUND_TRUTH,\n","      confidence        = None,\n","      format            = BBFormat.XYX2Y2\n","    )\n","    \n","    # detected\n","    det, conf = detected(\n","        preds,\n","        [ix, cls, 1, X1, Y1, X2, Y2],\n","        0.6\n","    )\n","    metadata.append({'image_id': ix,\n","                    'image_version': 'noisy',\n","                    'image_brightness_value': c_img['value'],\n","                    'image_brightness_interpretation': c_img['brightness'],\n","                    'image_rgb_avg_r': c_avg_img['rgb_avg_r'],\n","                    'image_rgb_avg_g': c_avg_img['rgb_avg_g'],\n","                    'image_rgb_avg_b': c_avg_img['rgb_avg_b'],\n","                    'image_hsv_avg_h': c_avg_img['hsv_avg_h'],\n","                    'image_hsv_avg_s': c_avg_img['hsv_avg_s'],\n","                    'image_hsv_avg_v': c_avg_img['hsv_avg_v'],\n","                    'object_brightness_value': c_obj['value'],\n","                    'object_brightness_interpretation': c_obj['brightness'],\n","                    'object_rgb_avg_r': c_avg_obj['rgb_avg_r'],\n","                    'object_rgb_avg_g': c_avg_obj['rgb_avg_g'],\n","                    'object_rgb_avg_b': c_avg_obj['rgb_avg_b'],\n","                    'object_hsv_avg_h': c_avg_obj['hsv_avg_h'],\n","                    'object_hsv_avg_s': c_avg_obj['hsv_avg_s'],\n","                    'object_hsv_avg_v': c_avg_obj['hsv_avg_v'],\n","                    'detected': det,\n","                    'confidence': conf,\n","                    'object_size': bb.get_area(),\n","                    'object_class': cls\n","                   }\n","                  )\n","###########################################################################################\n","  # bright image\n","\n","  new_image = PIL.ImageEnhance.Brightness(img_).enhance(2)\n","  bright_image = np.array(new_image) / 255\n","  bright_imgs = [torch.from_numpy(bright_image).permute(2, 0, 1)]\n","  bright_imgs = [torch.tensor(im).type(torch.FloatTensor).to(device) for im in bright_imgs]\n","  bright_img = bright_imgs[0]\n","\n","  c_img = image_light(bright_image)\n","  c_avg_img = avg_img_color(bright_image)\n","\n","\n","\n","  preds = []\n","  outputs = model(bright_imgs) \n","\n","  for i_output, output in enumerate(outputs):\n","    bbs, confs, labels = decode_output(output)\n","    for i_bb, bb in enumerate(bbs):\n","      X1, Y1, X2, Y2 = bb\n","      preds.append([ix, labels[i_bb], confs[i_bb], X1, Y1, X2, Y2])\n","\n","\n","  for i, box in enumerate(targets[0]['boxes']):\n","    cls = target2label[int(targets[0]['labels'][i])]\n","\n","    X1, Y1, X2, Y2 = box.numpy().astype(int)\n","\n","    # object frame\n","    obj_frame = bright_img[:3, Y1:Y2, X1:X2]\n","\n","    # colors\n","    c_obj = image_light(obj_frame)\n","    c_avg_obj = avg_img_color(obj_frame)\n","\n","    # Area\n","    bb = BoundingBox(\n","      image_name        = str(ix),\n","      class_id          = cls,\n","      coordinates       = list(box),\n","      type_coordinates  = CoordinatesType.ABSOLUTE,\n","      bb_type           = BBType.GROUND_TRUTH,\n","      confidence        = None,\n","      format            = BBFormat.XYX2Y2\n","    )\n","    \n","    # detected\n","    det, conf = detected(\n","        preds,\n","        [ix, cls, 1, X1, Y1, X2, Y2],\n","        0.6\n","    )\n","    metadata.append({'image_id': ix,\n","                    'image_version': 'bright',\n","                    'image_brightness_value': c_img['value'],\n","                    'image_brightness_interpretation': c_img['brightness'],\n","                    'image_rgb_avg_r': c_avg_img['rgb_avg_r'],\n","                    'image_rgb_avg_g': c_avg_img['rgb_avg_g'],\n","                    'image_rgb_avg_b': c_avg_img['rgb_avg_b'],\n","                    'image_hsv_avg_h': c_avg_img['hsv_avg_h'],\n","                    'image_hsv_avg_s': c_avg_img['hsv_avg_s'],\n","                    'image_hsv_avg_v': c_avg_img['hsv_avg_v'],\n","                    'object_brightness_value': c_obj['value'],\n","                    'object_brightness_interpretation': c_obj['brightness'],\n","                    'object_rgb_avg_r': c_avg_obj['rgb_avg_r'],\n","                    'object_rgb_avg_g': c_avg_obj['rgb_avg_g'],\n","                    'object_rgb_avg_b': c_avg_obj['rgb_avg_b'],\n","                    'object_hsv_avg_h': c_avg_obj['hsv_avg_h'],\n","                    'object_hsv_avg_s': c_avg_obj['hsv_avg_s'],\n","                    'object_hsv_avg_v': c_avg_obj['hsv_avg_v'],\n","                    'detected': det,\n","                    'confidence': conf,\n","                    'object_size': bb.get_area(),\n","                    'object_class': cls\n","                   }\n","                  )\n","    \n","###########################################################################################\n","  # contrast image\n","\n","  new_image = PIL.ImageEnhance.Contrast(img_).enhance(2)\n","  contrast_image = np.array(new_image) / 255\n","  contrast_imgs = [torch.from_numpy(contrast_image).permute(2, 0, 1)]\n","  contrast_imgs = [torch.tensor(im).type(torch.FloatTensor).to(device) for im in contrast_imgs]\n","  contrast_img = contrast_imgs[0]\n","\n","  c_img = image_light(contrast_image)\n","  c_avg_img = avg_img_color(contrast_image)\n","\n","\n","\n","  preds = []\n","  outputs = model(contrast_imgs)\n","\n","  for i_output, output in enumerate(outputs):\n","    bbs, confs, labels = decode_output(output)\n","    for i_bb, bb in enumerate(bbs):\n","      X1, Y1, X2, Y2 = bb\n","      preds.append([ix, labels[i_bb], confs[i_bb], X1, Y1, X2, Y2])\n","\n","\n","  for i, box in enumerate(targets[0]['boxes']):\n","    cls = target2label[int(targets[0]['labels'][i])]\n","\n","    X1, Y1, X2, Y2 = box.numpy().astype(int)\n","\n","    # object frame\n","    obj_frame = contrast_img[:3, Y1:Y2, X1:X2]\n","\n","    # colors\n","    c_obj = image_light(obj_frame)\n","    c_avg_obj = avg_img_color(obj_frame)\n","\n","    # Area\n","    bb = BoundingBox(\n","      image_name        = str(ix),\n","      class_id          = cls,\n","      coordinates       = list(box),\n","      type_coordinates  = CoordinatesType.ABSOLUTE,\n","      bb_type           = BBType.GROUND_TRUTH,\n","      confidence        = None,\n","      format            = BBFormat.XYX2Y2\n","    )\n","    \n","    # detected\n","    det, conf = detected(\n","        preds,\n","        [ix, cls, 1, X1, Y1, X2, Y2],\n","        0.6\n","    )\n","    metadata.append({'image_id': ix,\n","                    'image_version': 'contrast',\n","                    'image_brightness_value': c_img['value'],\n","                    'image_brightness_interpretation': c_img['brightness'],\n","                    'image_rgb_avg_r': c_avg_img['rgb_avg_r'],\n","                    'image_rgb_avg_g': c_avg_img['rgb_avg_g'],\n","                    'image_rgb_avg_b': c_avg_img['rgb_avg_b'],\n","                    'image_hsv_avg_h': c_avg_img['hsv_avg_h'],\n","                    'image_hsv_avg_s': c_avg_img['hsv_avg_s'],\n","                    'image_hsv_avg_v': c_avg_img['hsv_avg_v'],\n","                    'object_brightness_value': c_obj['value'],\n","                    'object_brightness_interpretation': c_obj['brightness'],\n","                    'object_rgb_avg_r': c_avg_obj['rgb_avg_r'],\n","                    'object_rgb_avg_g': c_avg_obj['rgb_avg_g'],\n","                    'object_rgb_avg_b': c_avg_obj['rgb_avg_b'],\n","                    'object_hsv_avg_h': c_avg_obj['hsv_avg_h'],\n","                    'object_hsv_avg_s': c_avg_obj['hsv_avg_s'],\n","                    'object_hsv_avg_v': c_avg_obj['hsv_avg_v'],\n","                    'detected': det,\n","                    'confidence': conf,\n","                    'object_size': bb.get_area(),\n","                    'object_class': cls\n","                   }\n","                  )\n","###########################################################################################\n","  # sharp image\n","\n","  new_image = PIL.ImageEnhance.Sharpness(img_).enhance(3)\n","  sharp_image = np.array(new_image) / 255\n","  sharp_imgs = [torch.from_numpy(sharp_image).permute(2, 0, 1)]\n","  sharp_imgs = [torch.tensor(im).type(torch.FloatTensor).to(device) for im in sharp_imgs]\n","  sharp_img = sharp_imgs[0]\n","\n","  c_img = image_light(sharp_image)\n","  c_avg_img = avg_img_color(sharp_image)\n","\n","\n","\n","  preds = []\n","  outputs = model(sharp_imgs)\n","\n","  for i_output, output in enumerate(outputs):\n","    bbs, confs, labels = decode_output(output)\n","    for i_bb, bb in enumerate(bbs):\n","      X1, Y1, X2, Y2 = bb\n","      preds.append([ix, labels[i_bb], confs[i_bb], X1, Y1, X2, Y2])\n","\n","\n","  for i, box in enumerate(targets[0]['boxes']):\n","    cls = target2label[int(targets[0]['labels'][i])]\n","\n","    X1, Y1, X2, Y2 = box.numpy().astype(int)\n","\n","    # object frame\n","    obj_frame = sharp_img[:3, Y1:Y2, X1:X2]\n","\n","    # colors\n","    c_obj = image_light(obj_frame)\n","    c_avg_obj = avg_img_color(obj_frame)\n","\n","    # Area\n","    bb = BoundingBox(\n","      image_name        = str(ix),\n","      class_id          = cls,\n","      coordinates       = list(box),\n","      type_coordinates  = CoordinatesType.ABSOLUTE,\n","      bb_type           = BBType.GROUND_TRUTH,\n","      confidence        = None,\n","      format            = BBFormat.XYX2Y2\n","    )\n","    \n","    # detected\n","    det, conf = detected(\n","        preds,\n","        [ix, cls, 1, X1, Y1, X2, Y2],\n","        0.6\n","    )\n","    metadata.append({'image_id': ix,\n","                    'image_version': 'sharp',\n","                    'image_brightness_value': c_img['value'],\n","                    'image_brightness_interpretation': c_img['brightness'],\n","                    'image_rgb_avg_r': c_avg_img['rgb_avg_r'],\n","                    'image_rgb_avg_g': c_avg_img['rgb_avg_g'],\n","                    'image_rgb_avg_b': c_avg_img['rgb_avg_b'],\n","                    'image_hsv_avg_h': c_avg_img['hsv_avg_h'],\n","                    'image_hsv_avg_s': c_avg_img['hsv_avg_s'],\n","                    'image_hsv_avg_v': c_avg_img['hsv_avg_v'],\n","                    'object_brightness_value': c_obj['value'],\n","                    'object_brightness_interpretation': c_obj['brightness'],\n","                    'object_rgb_avg_r': c_avg_obj['rgb_avg_r'],\n","                    'object_rgb_avg_g': c_avg_obj['rgb_avg_g'],\n","                    'object_rgb_avg_b': c_avg_obj['rgb_avg_b'],\n","                    'object_hsv_avg_h': c_avg_obj['hsv_avg_h'],\n","                    'object_hsv_avg_s': c_avg_obj['hsv_avg_s'],\n","                    'object_hsv_avg_v': c_avg_obj['hsv_avg_v'],\n","                    'detected': det,\n","                    'confidence': conf,\n","                    'object_size': bb.get_area(),\n","                    'object_class': cls\n","                   }\n","                  )\n","###########################################################################################\n","  # Dark image\n","\n","  new_image = PIL.ImageEnhance.Brightness(img_).enhance(0.4)\n","  dark_image = np.array(new_image) / 255\n","  dark_imgs = [torch.from_numpy(dark_image).permute(2, 0, 1)]\n","  dark_imgs = [torch.tensor(im).type(torch.FloatTensor).to(device) for im in dark_imgs]\n","  dark_img = dark_imgs[0]\n","\n","  c_img = image_light(dark_image)\n","  c_avg_img = avg_img_color(dark_image)\n","\n","\n","\n","  preds = []\n","  outputs = model(dark_imgs)\n","\n","  for i_output, output in enumerate(outputs):\n","    bbs, confs, labels = decode_output(output)\n","    for i_bb, bb in enumerate(bbs):\n","      X1, Y1, X2, Y2 = bb\n","      preds.append([ix, labels[i_bb], confs[i_bb], X1, Y1, X2, Y2])\n","\n","\n","  for i, box in enumerate(targets[0]['boxes']):\n","    cls = target2label[int(targets[0]['labels'][i])]\n","\n","    X1, Y1, X2, Y2 = box.numpy().astype(int)\n","\n","    # object frame\n","    obj_frame = dark_img[:3, Y1:Y2, X1:X2]\n","\n","    # colors\n","    c_obj = image_light(obj_frame)\n","    c_avg_obj = avg_img_color(obj_frame)\n","\n","    # Area\n","    bb = BoundingBox(\n","      image_name        = str(ix),\n","      class_id          = cls,\n","      coordinates       = list(box),\n","      type_coordinates  = CoordinatesType.ABSOLUTE,\n","      bb_type           = BBType.GROUND_TRUTH,\n","      confidence        = None,\n","      format            = BBFormat.XYX2Y2\n","    )\n","    \n","    # detected\n","    det, conf = detected(\n","        preds,\n","        [ix, cls, 1, X1, Y1, X2, Y2],\n","        0.6\n","    )\n","    metadata.append({'image_id': ix,\n","                    'image_version': 'dark',\n","                    'image_brightness_value': c_img['value'],\n","                    'image_brightness_interpretation': c_img['brightness'],\n","                    'image_rgb_avg_r': c_avg_img['rgb_avg_r'],\n","                    'image_rgb_avg_g': c_avg_img['rgb_avg_g'],\n","                    'image_rgb_avg_b': c_avg_img['rgb_avg_b'],\n","                    'image_hsv_avg_h': c_avg_img['hsv_avg_h'],\n","                    'image_hsv_avg_s': c_avg_img['hsv_avg_s'],\n","                    'image_hsv_avg_v': c_avg_img['hsv_avg_v'],\n","                    'object_brightness_value': c_obj['value'],\n","                    'object_brightness_interpretation': c_obj['brightness'],\n","                    'object_rgb_avg_r': c_avg_obj['rgb_avg_r'],\n","                    'object_rgb_avg_g': c_avg_obj['rgb_avg_g'],\n","                    'object_rgb_avg_b': c_avg_obj['rgb_avg_b'],\n","                    'object_hsv_avg_h': c_avg_obj['hsv_avg_h'],\n","                    'object_hsv_avg_s': c_avg_obj['hsv_avg_s'],\n","                    'object_hsv_avg_v': c_avg_obj['hsv_avg_v'],\n","                    'detected': det,\n","                    'confidence': conf,\n","                    'object_size': bb.get_area(),\n","                    'object_class': cls\n","                   }\n","                  )\n","  printProgressBar(ix + 1, n_items, prefix = 'Progress:', suffix = 'Complete', length = 50)"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":333},"executionInfo":{"elapsed":17165,"status":"ok","timestamp":1650294013079,"user":{"displayName":"Bastian B.","userId":"10033977703569380654"},"user_tz":-120},"id":"BAvFYUomVRgl","outputId":"52065dd8-0783-4e6a-ef6e-d85f75f35d87"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["       image_id image_version  image_brightness_value  \\\n","33698      3044        bright              134.776908   \n","33699      3044      contrast               95.425101   \n","33700      3044      contrast               95.425101   \n","33701      3044         sharp               82.461668   \n","33702      3044         sharp               82.461668   \n","33703      3044          dark               32.575656   \n","33704      3044          dark               32.575656   \n","\n","      image_brightness_interpretation  image_rgb_avg_r  image_rgb_avg_g  \\\n","33698                           light         0.552707         0.519490   \n","33699                            dark         0.405319         0.362328   \n","33700                            dark         0.405319         0.362328   \n","33701                            dark         0.343913         0.316154   \n","33702                            dark         0.343913         0.316154   \n","33703                            dark         0.136002         0.124840   \n","33704                            dark         0.136002         0.124840   \n","\n","       image_rgb_avg_b  image_hsv_avg_h  image_hsv_avg_s  image_hsv_avg_v  \\\n","33698         0.512211         0.254606         0.208871         0.568104   \n","33699         0.354209         0.183127         0.219872         0.419370   \n","33700         0.354209         0.183127         0.219872         0.419370   \n","33701         0.307026         0.338734         0.255500         0.356855   \n","33702         0.307026         0.338734         0.255500         0.356855   \n","33703         0.121187         0.313693         0.260449         0.140653   \n","33704         0.121187         0.313693         0.260449         0.140653   \n","\n","       object_brightness_value object_brightness_interpretation  \\\n","33698               219.471943                            light   \n","33699               198.343621                            light   \n","33700               175.065043                            light   \n","33701               151.929901                            light   \n","33702               137.296597                            light   \n","33703                60.404664                             dark   \n","33704                54.514213                             dark   \n","\n","       object_rgb_avg_r  object_rgb_avg_g  object_rgb_avg_b  object_hsv_avg_h  \\\n","33698          0.900724          0.844611          0.839133          0.215730   \n","33699          0.790550          0.769863          0.786081          0.294754   \n","33700          0.755075          0.657424          0.657277          0.318293   \n","33701          0.607832          0.591539          0.586746          0.379261   \n","33702          0.574613          0.523310          0.521784          0.416595   \n","33703          0.241760          0.235164          0.233139          0.361929   \n","33704          0.228268          0.207731          0.207138          0.402688   \n","\n","       object_hsv_avg_s  object_hsv_avg_v  detected  confidence  object_size  \\\n","33698          0.116299          0.918348     False         0.0      10486.0   \n","33699          0.135345          0.818334     False         0.0       2240.0   \n","33700          0.271466          0.785582     False         0.0      10486.0   \n","33701          0.131465          0.630860     False         0.0       2240.0   \n","33702          0.209666          0.603817     False         0.0      10486.0   \n","33703          0.113127          0.249445     False         0.0       2240.0   \n","33704          0.190001          0.238191     False         0.0      10486.0   \n","\n","      object_class  \n","33698          Bus  \n","33699          Bus  \n","33700          Bus  \n","33701          Bus  \n","33702          Bus  \n","33703          Bus  \n","33704          Bus  "],"text/html":["\n","  <div id=\"df-22333509-1ef2-4f08-8d81-fb12959752b4\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>image_id</th>\n","      <th>image_version</th>\n","      <th>image_brightness_value</th>\n","      <th>image_brightness_interpretation</th>\n","      <th>image_rgb_avg_r</th>\n","      <th>image_rgb_avg_g</th>\n","      <th>image_rgb_avg_b</th>\n","      <th>image_hsv_avg_h</th>\n","      <th>image_hsv_avg_s</th>\n","      <th>image_hsv_avg_v</th>\n","      <th>object_brightness_value</th>\n","      <th>object_brightness_interpretation</th>\n","      <th>object_rgb_avg_r</th>\n","      <th>object_rgb_avg_g</th>\n","      <th>object_rgb_avg_b</th>\n","      <th>object_hsv_avg_h</th>\n","      <th>object_hsv_avg_s</th>\n","      <th>object_hsv_avg_v</th>\n","      <th>detected</th>\n","      <th>confidence</th>\n","      <th>object_size</th>\n","      <th>object_class</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>33698</th>\n","      <td>3044</td>\n","      <td>bright</td>\n","      <td>134.776908</td>\n","      <td>light</td>\n","      <td>0.552707</td>\n","      <td>0.519490</td>\n","      <td>0.512211</td>\n","      <td>0.254606</td>\n","      <td>0.208871</td>\n","      <td>0.568104</td>\n","      <td>219.471943</td>\n","      <td>light</td>\n","      <td>0.900724</td>\n","      <td>0.844611</td>\n","      <td>0.839133</td>\n","      <td>0.215730</td>\n","      <td>0.116299</td>\n","      <td>0.918348</td>\n","      <td>False</td>\n","      <td>0.0</td>\n","      <td>10486.0</td>\n","      <td>Bus</td>\n","    </tr>\n","    <tr>\n","      <th>33699</th>\n","      <td>3044</td>\n","      <td>contrast</td>\n","      <td>95.425101</td>\n","      <td>dark</td>\n","      <td>0.405319</td>\n","      <td>0.362328</td>\n","      <td>0.354209</td>\n","      <td>0.183127</td>\n","      <td>0.219872</td>\n","      <td>0.419370</td>\n","      <td>198.343621</td>\n","      <td>light</td>\n","      <td>0.790550</td>\n","      <td>0.769863</td>\n","      <td>0.786081</td>\n","      <td>0.294754</td>\n","      <td>0.135345</td>\n","      <td>0.818334</td>\n","      <td>False</td>\n","      <td>0.0</td>\n","      <td>2240.0</td>\n","      <td>Bus</td>\n","    </tr>\n","    <tr>\n","      <th>33700</th>\n","      <td>3044</td>\n","      <td>contrast</td>\n","      <td>95.425101</td>\n","      <td>dark</td>\n","      <td>0.405319</td>\n","      <td>0.362328</td>\n","      <td>0.354209</td>\n","      <td>0.183127</td>\n","      <td>0.219872</td>\n","      <td>0.419370</td>\n","      <td>175.065043</td>\n","      <td>light</td>\n","      <td>0.755075</td>\n","      <td>0.657424</td>\n","      <td>0.657277</td>\n","      <td>0.318293</td>\n","      <td>0.271466</td>\n","      <td>0.785582</td>\n","      <td>False</td>\n","      <td>0.0</td>\n","      <td>10486.0</td>\n","      <td>Bus</td>\n","    </tr>\n","    <tr>\n","      <th>33701</th>\n","      <td>3044</td>\n","      <td>sharp</td>\n","      <td>82.461668</td>\n","      <td>dark</td>\n","      <td>0.343913</td>\n","      <td>0.316154</td>\n","      <td>0.307026</td>\n","      <td>0.338734</td>\n","      <td>0.255500</td>\n","      <td>0.356855</td>\n","      <td>151.929901</td>\n","      <td>light</td>\n","      <td>0.607832</td>\n","      <td>0.591539</td>\n","      <td>0.586746</td>\n","      <td>0.379261</td>\n","      <td>0.131465</td>\n","      <td>0.630860</td>\n","      <td>False</td>\n","      <td>0.0</td>\n","      <td>2240.0</td>\n","      <td>Bus</td>\n","    </tr>\n","    <tr>\n","      <th>33702</th>\n","      <td>3044</td>\n","      <td>sharp</td>\n","      <td>82.461668</td>\n","      <td>dark</td>\n","      <td>0.343913</td>\n","      <td>0.316154</td>\n","      <td>0.307026</td>\n","      <td>0.338734</td>\n","      <td>0.255500</td>\n","      <td>0.356855</td>\n","      <td>137.296597</td>\n","      <td>light</td>\n","      <td>0.574613</td>\n","      <td>0.523310</td>\n","      <td>0.521784</td>\n","      <td>0.416595</td>\n","      <td>0.209666</td>\n","      <td>0.603817</td>\n","      <td>False</td>\n","      <td>0.0</td>\n","      <td>10486.0</td>\n","      <td>Bus</td>\n","    </tr>\n","    <tr>\n","      <th>33703</th>\n","      <td>3044</td>\n","      <td>dark</td>\n","      <td>32.575656</td>\n","      <td>dark</td>\n","      <td>0.136002</td>\n","      <td>0.124840</td>\n","      <td>0.121187</td>\n","      <td>0.313693</td>\n","      <td>0.260449</td>\n","      <td>0.140653</td>\n","      <td>60.404664</td>\n","      <td>dark</td>\n","      <td>0.241760</td>\n","      <td>0.235164</td>\n","      <td>0.233139</td>\n","      <td>0.361929</td>\n","      <td>0.113127</td>\n","      <td>0.249445</td>\n","      <td>False</td>\n","      <td>0.0</td>\n","      <td>2240.0</td>\n","      <td>Bus</td>\n","    </tr>\n","    <tr>\n","      <th>33704</th>\n","      <td>3044</td>\n","      <td>dark</td>\n","      <td>32.575656</td>\n","      <td>dark</td>\n","      <td>0.136002</td>\n","      <td>0.124840</td>\n","      <td>0.121187</td>\n","      <td>0.313693</td>\n","      <td>0.260449</td>\n","      <td>0.140653</td>\n","      <td>54.514213</td>\n","      <td>dark</td>\n","      <td>0.228268</td>\n","      <td>0.207731</td>\n","      <td>0.207138</td>\n","      <td>0.402688</td>\n","      <td>0.190001</td>\n","      <td>0.238191</td>\n","      <td>False</td>\n","      <td>0.0</td>\n","      <td>10486.0</td>\n","      <td>Bus</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-22333509-1ef2-4f08-8d81-fb12959752b4')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-22333509-1ef2-4f08-8d81-fb12959752b4 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-22333509-1ef2-4f08-8d81-fb12959752b4');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":24}],"source":["df = pd.DataFrame.from_dict(metadata)\n","df.to_excel(join(OUTPUT_REPORTS, output_testing_report))\n","df.tail(7)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"Testing_FasterRCNN_MobileNetV3-Large-320.ipynb","provenance":[],"authorship_tag":"ABX9TyMXGGqaFSlojACWSy/1bVSI"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"7892fbb940404d9286a17221d84578a3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e4aeb3050baf474688c0b6760a2a3418","IPY_MODEL_58ddc9790e57486dadaba01c4d2b8182","IPY_MODEL_b9ded4a8dc9240ba934fafc83710f673"],"layout":"IPY_MODEL_09dc7f97c0ed47448fdb5e7c4f5f208a"}},"e4aeb3050baf474688c0b6760a2a3418":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2b0cad322f6840f7a6b496acbcd20e79","placeholder":"​","style":"IPY_MODEL_89732667f6c24fe285c915af1bec620f","value":"100%"}},"58ddc9790e57486dadaba01c4d2b8182":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b7e8a5f728474e1a9bbaa247b6ed4274","max":22139423,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bcf029e87c644e8a882fa22f522e98af","value":22139423}},"b9ded4a8dc9240ba934fafc83710f673":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a082ade322c948dd90e5e10dd781e07c","placeholder":"​","style":"IPY_MODEL_625efa976af84705a271e2544a34fb60","value":" 21.1M/21.1M [00:00&lt;00:00, 52.1MB/s]"}},"09dc7f97c0ed47448fdb5e7c4f5f208a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2b0cad322f6840f7a6b496acbcd20e79":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"89732667f6c24fe285c915af1bec620f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b7e8a5f728474e1a9bbaa247b6ed4274":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bcf029e87c644e8a882fa22f522e98af":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a082ade322c948dd90e5e10dd781e07c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"625efa976af84705a271e2544a34fb60":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}